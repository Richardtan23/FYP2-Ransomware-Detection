{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "063528d0-8abc-4c16-96ef-caa6aaa273b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "PROJECT = Path.cwd()\n",
    "if PROJECT.name == \"notebooks\":\n",
    "    PROJECT = PROJECT.parent  \n",
    "\n",
    "DATASETS = PROJECT / \"datasets\"\n",
    "STATIC_PATH = DATASETS / \"static\" / \"ransomware_detection.csv\"      \n",
    "BEHAV_PATH  = DATASETS / \"behavioral\" / \"ugransom.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ba29829-2db2-4a2b-afcc-c1dfa89f2727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('C:/Users/richa/OneDrive/Documents/FYP2/datasets/static/ransomware_detection.csv'),\n",
       " WindowsPath('C:/Users/richa/OneDrive/Documents/FYP2/datasets/behavioral/ugransom.csv'),\n",
       " True,\n",
       " True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STATIC_PATH, BEHAV_PATH, STATIC_PATH.exists(), BEHAV_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b3d6101-7510-40a1-9e5e-ad758b9e53e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('C:/Users/richa/OneDrive/Documents/FYP2/datasets/static/ransomware_detection.csv'),\n",
       " WindowsPath('C:/Users/richa/OneDrive/Documents/FYP2/datasets/behavioral/ugransom.csv'),\n",
       " True,\n",
       " True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STATIC_PATH, BEHAV_PATH, STATIC_PATH.exists(), BEHAV_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eba28af-45a8-4b20-baaa-6288362041da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_static = pd.read_csv(STATIC_PATH, low_memory=False)\n",
    "df_behav  = pd.read_csv(BEHAV_PATH,  low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e192bde8-7f93-4b39-91f6-53d9456e8436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATIC shape: (62485, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileName</th>\n",
       "      <th>md5Hash</th>\n",
       "      <th>Machine</th>\n",
       "      <th>DebugSize</th>\n",
       "      <th>DebugRVA</th>\n",
       "      <th>MajorImageVersion</th>\n",
       "      <th>MajorOSVersion</th>\n",
       "      <th>ExportRVA</th>\n",
       "      <th>ExportSize</th>\n",
       "      <th>IatVRA</th>\n",
       "      <th>MajorLinkerVersion</th>\n",
       "      <th>MinorLinkerVersion</th>\n",
       "      <th>NumberOfSections</th>\n",
       "      <th>SizeOfStackReserve</th>\n",
       "      <th>DllCharacteristics</th>\n",
       "      <th>ResourceSize</th>\n",
       "      <th>BitcoinAddresses</th>\n",
       "      <th>Benign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0124e21d-018c-4ce0-92a3-b9e205a76bc0.dll</td>\n",
       "      <td>79755c51e413ed3c6be4635fd729a6e1</td>\n",
       "      <td>332</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8192</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1048576</td>\n",
       "      <td>34112</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>05c8318f98a5d301d80000009c316005.vertdll.dll</td>\n",
       "      <td>95e19f3657d34a432eada93221b0ea16</td>\n",
       "      <td>34404</td>\n",
       "      <td>84</td>\n",
       "      <td>121728</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>126576</td>\n",
       "      <td>4930</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>262144</td>\n",
       "      <td>16864</td>\n",
       "      <td>1024</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>06054fba-5619-4a86-a861-ffb0464bef5d.dll</td>\n",
       "      <td>85c32641d77a54e19ba8ea4ab305c791</td>\n",
       "      <td>332</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8192</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1048576</td>\n",
       "      <td>34112</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       FileName  \\\n",
       "0      0124e21d-018c-4ce0-92a3-b9e205a76bc0.dll   \n",
       "1  05c8318f98a5d301d80000009c316005.vertdll.dll   \n",
       "2      06054fba-5619-4a86-a861-ffb0464bef5d.dll   \n",
       "\n",
       "                            md5Hash  Machine  DebugSize  DebugRVA  \\\n",
       "0  79755c51e413ed3c6be4635fd729a6e1      332          0         0   \n",
       "1  95e19f3657d34a432eada93221b0ea16    34404         84    121728   \n",
       "2  85c32641d77a54e19ba8ea4ab305c791      332          0         0   \n",
       "\n",
       "   MajorImageVersion  MajorOSVersion  ExportRVA  ExportSize  IatVRA  \\\n",
       "0                  0               4          0           0    8192   \n",
       "1                 10              10     126576        4930       0   \n",
       "2                  0               4          0           0    8192   \n",
       "\n",
       "   MajorLinkerVersion  MinorLinkerVersion  NumberOfSections  \\\n",
       "0                   8                   0                 3   \n",
       "1                  14                  10                 8   \n",
       "2                   8                   0                 3   \n",
       "\n",
       "   SizeOfStackReserve  DllCharacteristics  ResourceSize  BitcoinAddresses  \\\n",
       "0             1048576               34112           672                 0   \n",
       "1              262144               16864          1024                 0   \n",
       "2             1048576               34112           672                 0   \n",
       "\n",
       "   Benign  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEHAV shape: (149043, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Protcol</th>\n",
       "      <th>Flag</th>\n",
       "      <th>Family</th>\n",
       "      <th>Clusters</th>\n",
       "      <th>SeddAddress</th>\n",
       "      <th>ExpAddress</th>\n",
       "      <th>BTC</th>\n",
       "      <th>USD</th>\n",
       "      <th>Netflow_Bytes</th>\n",
       "      <th>IPaddress</th>\n",
       "      <th>Threats</th>\n",
       "      <th>Port</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>TCP</td>\n",
       "      <td>A</td>\n",
       "      <td>WannaCry</td>\n",
       "      <td>1</td>\n",
       "      <td>1DA11mPS</td>\n",
       "      <td>1BonuSr7</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>Bonet</td>\n",
       "      <td>5061</td>\n",
       "      <td>SS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40</td>\n",
       "      <td>TCP</td>\n",
       "      <td>A</td>\n",
       "      <td>WannaCry</td>\n",
       "      <td>1</td>\n",
       "      <td>1DA11mPS</td>\n",
       "      <td>1BonuSr7</td>\n",
       "      <td>1</td>\n",
       "      <td>504</td>\n",
       "      <td>8</td>\n",
       "      <td>A</td>\n",
       "      <td>Bonet</td>\n",
       "      <td>5061</td>\n",
       "      <td>SS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>TCP</td>\n",
       "      <td>A</td>\n",
       "      <td>WannaCry</td>\n",
       "      <td>1</td>\n",
       "      <td>1DA11mPS</td>\n",
       "      <td>1BonuSr7</td>\n",
       "      <td>1</td>\n",
       "      <td>508</td>\n",
       "      <td>7</td>\n",
       "      <td>A</td>\n",
       "      <td>Bonet</td>\n",
       "      <td>5061</td>\n",
       "      <td>SS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time Protcol Flag    Family  Clusters SeddAddress ExpAddress  BTC  USD  \\\n",
       "0    50     TCP    A  WannaCry         1    1DA11mPS   1BonuSr7    1  500   \n",
       "1    40     TCP    A  WannaCry         1    1DA11mPS   1BonuSr7    1  504   \n",
       "2    30     TCP    A  WannaCry         1    1DA11mPS   1BonuSr7    1  508   \n",
       "\n",
       "   Netflow_Bytes IPaddress Threats  Port Prediction  \n",
       "0              5         A   Bonet  5061         SS  \n",
       "1              8         A   Bonet  5061         SS  \n",
       "2              7         A   Bonet  5061         SS  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'python' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBEHAV shape:\u001b[39m\u001b[33m\"\u001b[39m, df_behav.shape)\n\u001b[32m      4\u001b[39m display(df_behav.head(\u001b[32m3\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mpython\u001b[49m\n\u001b[32m      6\u001b[39m Copy\n\u001b[32m      7\u001b[39m Edit\n",
      "\u001b[31mNameError\u001b[39m: name 'python' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"STATIC shape:\", df_static.shape)\n",
    "display(df_static.head(3))\n",
    "print(\"\\nBEHAV shape:\", df_behav.shape)\n",
    "display(df_behav.head(3))\n",
    "python\n",
    "Copy\n",
    "Edit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f12e765-c049-4357-a0ee-663ab35f428e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATIC shape: (62485, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileName</th>\n",
       "      <th>md5Hash</th>\n",
       "      <th>Machine</th>\n",
       "      <th>DebugSize</th>\n",
       "      <th>DebugRVA</th>\n",
       "      <th>MajorImageVersion</th>\n",
       "      <th>MajorOSVersion</th>\n",
       "      <th>ExportRVA</th>\n",
       "      <th>ExportSize</th>\n",
       "      <th>IatVRA</th>\n",
       "      <th>MajorLinkerVersion</th>\n",
       "      <th>MinorLinkerVersion</th>\n",
       "      <th>NumberOfSections</th>\n",
       "      <th>SizeOfStackReserve</th>\n",
       "      <th>DllCharacteristics</th>\n",
       "      <th>ResourceSize</th>\n",
       "      <th>BitcoinAddresses</th>\n",
       "      <th>Benign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0124e21d-018c-4ce0-92a3-b9e205a76bc0.dll</td>\n",
       "      <td>79755c51e413ed3c6be4635fd729a6e1</td>\n",
       "      <td>332</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8192</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1048576</td>\n",
       "      <td>34112</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>05c8318f98a5d301d80000009c316005.vertdll.dll</td>\n",
       "      <td>95e19f3657d34a432eada93221b0ea16</td>\n",
       "      <td>34404</td>\n",
       "      <td>84</td>\n",
       "      <td>121728</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>126576</td>\n",
       "      <td>4930</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>262144</td>\n",
       "      <td>16864</td>\n",
       "      <td>1024</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>06054fba-5619-4a86-a861-ffb0464bef5d.dll</td>\n",
       "      <td>85c32641d77a54e19ba8ea4ab305c791</td>\n",
       "      <td>332</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8192</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1048576</td>\n",
       "      <td>34112</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       FileName  \\\n",
       "0      0124e21d-018c-4ce0-92a3-b9e205a76bc0.dll   \n",
       "1  05c8318f98a5d301d80000009c316005.vertdll.dll   \n",
       "2      06054fba-5619-4a86-a861-ffb0464bef5d.dll   \n",
       "\n",
       "                            md5Hash  Machine  DebugSize  DebugRVA  \\\n",
       "0  79755c51e413ed3c6be4635fd729a6e1      332          0         0   \n",
       "1  95e19f3657d34a432eada93221b0ea16    34404         84    121728   \n",
       "2  85c32641d77a54e19ba8ea4ab305c791      332          0         0   \n",
       "\n",
       "   MajorImageVersion  MajorOSVersion  ExportRVA  ExportSize  IatVRA  \\\n",
       "0                  0               4          0           0    8192   \n",
       "1                 10              10     126576        4930       0   \n",
       "2                  0               4          0           0    8192   \n",
       "\n",
       "   MajorLinkerVersion  MinorLinkerVersion  NumberOfSections  \\\n",
       "0                   8                   0                 3   \n",
       "1                  14                  10                 8   \n",
       "2                   8                   0                 3   \n",
       "\n",
       "   SizeOfStackReserve  DllCharacteristics  ResourceSize  BitcoinAddresses  \\\n",
       "0             1048576               34112           672                 0   \n",
       "1              262144               16864          1024                 0   \n",
       "2             1048576               34112           672                 0   \n",
       "\n",
       "   Benign  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEHAV shape: (149043, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Protcol</th>\n",
       "      <th>Flag</th>\n",
       "      <th>Family</th>\n",
       "      <th>Clusters</th>\n",
       "      <th>SeddAddress</th>\n",
       "      <th>ExpAddress</th>\n",
       "      <th>BTC</th>\n",
       "      <th>USD</th>\n",
       "      <th>Netflow_Bytes</th>\n",
       "      <th>IPaddress</th>\n",
       "      <th>Threats</th>\n",
       "      <th>Port</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>TCP</td>\n",
       "      <td>A</td>\n",
       "      <td>WannaCry</td>\n",
       "      <td>1</td>\n",
       "      <td>1DA11mPS</td>\n",
       "      <td>1BonuSr7</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>Bonet</td>\n",
       "      <td>5061</td>\n",
       "      <td>SS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40</td>\n",
       "      <td>TCP</td>\n",
       "      <td>A</td>\n",
       "      <td>WannaCry</td>\n",
       "      <td>1</td>\n",
       "      <td>1DA11mPS</td>\n",
       "      <td>1BonuSr7</td>\n",
       "      <td>1</td>\n",
       "      <td>504</td>\n",
       "      <td>8</td>\n",
       "      <td>A</td>\n",
       "      <td>Bonet</td>\n",
       "      <td>5061</td>\n",
       "      <td>SS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>TCP</td>\n",
       "      <td>A</td>\n",
       "      <td>WannaCry</td>\n",
       "      <td>1</td>\n",
       "      <td>1DA11mPS</td>\n",
       "      <td>1BonuSr7</td>\n",
       "      <td>1</td>\n",
       "      <td>508</td>\n",
       "      <td>7</td>\n",
       "      <td>A</td>\n",
       "      <td>Bonet</td>\n",
       "      <td>5061</td>\n",
       "      <td>SS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time Protcol Flag    Family  Clusters SeddAddress ExpAddress  BTC  USD  \\\n",
       "0    50     TCP    A  WannaCry         1    1DA11mPS   1BonuSr7    1  500   \n",
       "1    40     TCP    A  WannaCry         1    1DA11mPS   1BonuSr7    1  504   \n",
       "2    30     TCP    A  WannaCry         1    1DA11mPS   1BonuSr7    1  508   \n",
       "\n",
       "   Netflow_Bytes IPaddress Threats  Port Prediction  \n",
       "0              5         A   Bonet  5061         SS  \n",
       "1              8         A   Bonet  5061         SS  \n",
       "2              7         A   Bonet  5061         SS  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"STATIC shape:\", df_static.shape)\n",
    "display(df_static.head(3))\n",
    "print(\"\\nBEHAV shape:\", df_behav.shape)\n",
    "display(df_behav.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb59bb51-211f-4b9a-ac0f-6fc7d97766b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_report(df, name):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    miss = df.isna().mean().sort_values(ascending=False)\n",
    "    print(\"Top 10 missing columns:\\n\", miss.head(10))\n",
    "    print(\"Duplicate rows:\", df.duplicated().sum())\n",
    "    display(df.describe(include=\"all\").T.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c5aee8f-2280-4304-b7f5-68d5ede9fca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STATIC (PE features) ===\n",
      "Top 10 missing columns:\n",
      " FileName             0.0\n",
      "md5Hash              0.0\n",
      "Machine              0.0\n",
      "DebugSize            0.0\n",
      "DebugRVA             0.0\n",
      "MajorImageVersion    0.0\n",
      "MajorOSVersion       0.0\n",
      "ExportRVA            0.0\n",
      "ExportSize           0.0\n",
      "IatVRA               0.0\n",
      "dtype: float64\n",
      "Duplicate rows: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FileName</th>\n",
       "      <td>62485</td>\n",
       "      <td>62485</td>\n",
       "      <td>0124e21d-018c-4ce0-92a3-b9e205a76bc0.dll</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>md5Hash</th>\n",
       "      <td>62485</td>\n",
       "      <td>62485</td>\n",
       "      <td>79755c51e413ed3c6be4635fd729a6e1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Machine</th>\n",
       "      <td>62485.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6756.472657</td>\n",
       "      <td>13345.499919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>43620.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DebugSize</th>\n",
       "      <td>62485.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25870.477459</td>\n",
       "      <td>6461396.266999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1615155235.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DebugRVA</th>\n",
       "      <td>62485.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154161.114027</td>\n",
       "      <td>1903142.499245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12832.0</td>\n",
       "      <td>285212672.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MajorImageVersion</th>\n",
       "      <td>62485.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.785997</td>\n",
       "      <td>1114.068244</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>63325.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MajorOSVersion</th>\n",
       "      <td>62485.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.417524</td>\n",
       "      <td>2.543697</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExportRVA</th>\n",
       "      <td>62485.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>895318.632616</td>\n",
       "      <td>37795267.338042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28752.0</td>\n",
       "      <td>2147483648.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExportSize</th>\n",
       "      <td>62485.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>409462.345427</td>\n",
       "      <td>28518203.462559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>2415919104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IatVRA</th>\n",
       "      <td>62485.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>146631.139634</td>\n",
       "      <td>1124629.726753</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>8520.0</td>\n",
       "      <td>65536.0</td>\n",
       "      <td>66154496.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     count unique                                       top  \\\n",
       "FileName             62485  62485  0124e21d-018c-4ce0-92a3-b9e205a76bc0.dll   \n",
       "md5Hash              62485  62485          79755c51e413ed3c6be4635fd729a6e1   \n",
       "Machine            62485.0    NaN                                       NaN   \n",
       "DebugSize          62485.0    NaN                                       NaN   \n",
       "DebugRVA           62485.0    NaN                                       NaN   \n",
       "MajorImageVersion  62485.0    NaN                                       NaN   \n",
       "MajorOSVersion     62485.0    NaN                                       NaN   \n",
       "ExportRVA          62485.0    NaN                                       NaN   \n",
       "ExportSize         62485.0    NaN                                       NaN   \n",
       "IatVRA             62485.0    NaN                                       NaN   \n",
       "\n",
       "                  freq           mean              std  min     25%     50%  \\\n",
       "FileName             1            NaN              NaN  NaN     NaN     NaN   \n",
       "md5Hash              1            NaN              NaN  NaN     NaN     NaN   \n",
       "Machine            NaN    6756.472657     13345.499919  0.0   332.0   332.0   \n",
       "DebugSize          NaN   25870.477459   6461396.266999  0.0     0.0     0.0   \n",
       "DebugRVA           NaN  154161.114027   1903142.499245  0.0     0.0     0.0   \n",
       "MajorImageVersion  NaN      58.785997      1114.068244  0.0     0.0     0.0   \n",
       "MajorOSVersion     NaN       5.417524         2.543697  0.0     4.0     5.0   \n",
       "ExportRVA          NaN  895318.632616  37795267.338042  0.0     0.0     0.0   \n",
       "ExportSize         NaN  409462.345427  28518203.462559  0.0     0.0     0.0   \n",
       "IatVRA             NaN  146631.139634   1124629.726753  0.0  4096.0  8520.0   \n",
       "\n",
       "                       75%           max  \n",
       "FileName               NaN           NaN  \n",
       "md5Hash                NaN           NaN  \n",
       "Machine              332.0       43620.0  \n",
       "DebugSize             28.0  1615155235.0  \n",
       "DebugRVA           12832.0   285212672.0  \n",
       "MajorImageVersion      6.0       63325.0  \n",
       "MajorOSVersion         6.0         260.0  \n",
       "ExportRVA          28752.0  2147483648.0  \n",
       "ExportSize           104.0  2415919104.0  \n",
       "IatVRA             65536.0    66154496.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BEHAV (UGRansome) ===\n",
      "Top 10 missing columns:\n",
      " Time             0.0\n",
      "Protcol          0.0\n",
      "Flag             0.0\n",
      "Family           0.0\n",
      "Clusters         0.0\n",
      "SeddAddress      0.0\n",
      "ExpAddress       0.0\n",
      "BTC              0.0\n",
      "USD              0.0\n",
      "Netflow_Bytes    0.0\n",
      "dtype: float64\n",
      "Duplicate rows: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <td>149043.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.466979</td>\n",
       "      <td>15.883598</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Protcol</th>\n",
       "      <td>149043</td>\n",
       "      <td>3</td>\n",
       "      <td>TCP</td>\n",
       "      <td>68115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flag</th>\n",
       "      <td>149043</td>\n",
       "      <td>9</td>\n",
       "      <td>AF</td>\n",
       "      <td>53942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Family</th>\n",
       "      <td>149043</td>\n",
       "      <td>17</td>\n",
       "      <td>Locky</td>\n",
       "      <td>25062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clusters</th>\n",
       "      <td>149043.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.349295</td>\n",
       "      <td>2.828759</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SeddAddress</th>\n",
       "      <td>149043</td>\n",
       "      <td>6</td>\n",
       "      <td>1DA11mPS</td>\n",
       "      <td>58453</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExpAddress</th>\n",
       "      <td>149043</td>\n",
       "      <td>7</td>\n",
       "      <td>1DiCeTjB</td>\n",
       "      <td>56311</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTC</th>\n",
       "      <td>149043.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.554605</td>\n",
       "      <td>101.447102</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1980.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USD</th>\n",
       "      <td>149043.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14863.441114</td>\n",
       "      <td>26849.434659</td>\n",
       "      <td>1.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>4321.0</td>\n",
       "      <td>18454.0</td>\n",
       "      <td>126379.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Netflow_Bytes</th>\n",
       "      <td>149043.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021.278651</td>\n",
       "      <td>2271.420987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>1031.0</td>\n",
       "      <td>3188.0</td>\n",
       "      <td>12360.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  count unique       top   freq          mean           std  \\\n",
       "Time           149043.0    NaN       NaN    NaN     21.466979     15.883598   \n",
       "Protcol          149043      3       TCP  68115           NaN           NaN   \n",
       "Flag             149043      9        AF  53942           NaN           NaN   \n",
       "Family           149043     17     Locky  25062           NaN           NaN   \n",
       "Clusters       149043.0    NaN       NaN    NaN      2.349295      2.828759   \n",
       "SeddAddress      149043      6  1DA11mPS  58453           NaN           NaN   \n",
       "ExpAddress       149043      7  1DiCeTjB  56311           NaN           NaN   \n",
       "BTC            149043.0    NaN       NaN    NaN     30.554605    101.447102   \n",
       "USD            149043.0    NaN       NaN    NaN  14863.441114  26849.434659   \n",
       "Netflow_Bytes  149043.0    NaN       NaN    NaN   2021.278651   2271.420987   \n",
       "\n",
       "                min    25%     50%      75%       max  \n",
       "Time          -10.0    8.0    19.0     32.0      96.0  \n",
       "Protcol         NaN    NaN     NaN      NaN       NaN  \n",
       "Flag            NaN    NaN     NaN      NaN       NaN  \n",
       "Family          NaN    NaN     NaN      NaN       NaN  \n",
       "Clusters        1.0    1.0     1.0      2.0      12.0  \n",
       "SeddAddress     NaN    NaN     NaN      NaN       NaN  \n",
       "ExpAddress      NaN    NaN     NaN      NaN       NaN  \n",
       "BTC             1.0    8.0    13.0     22.0    1980.0  \n",
       "USD             1.0  512.0  4321.0  18454.0  126379.0  \n",
       "Netflow_Bytes   1.0  353.0  1031.0   3188.0   12360.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quick_report(df_static, \"STATIC (PE features)\")\n",
    "quick_report(df_behav,  \"BEHAV (UGRansome)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd34e5cc-9bc8-49e1-a501-45b79654f8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign\n",
      "0    35367\n",
      "1    27118\n",
      "Name: count, dtype: int64\n",
      "Static benign ratio: 0.43399215811794833\n"
     ]
    }
   ],
   "source": [
    "label_static = \"Benign\"\n",
    "if label_static in df_static.columns:\n",
    "    print(df_static[label_static].value_counts(dropna=False))\n",
    "    print(\"Static benign ratio:\", df_static[label_static].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5124f439-10cf-4282-a3fb-8b691527d2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed behavioral columns to expected schema.\n",
      "\n",
      "BEHAV Prediction distribution:\n",
      "Prediction\n",
      "S     66380\n",
      "A     42561\n",
      "SS    40102\n",
      "Name: count, dtype: int64\n",
      "\n",
      "BEHAV Family distribution:\n",
      "Family\n",
      "Locky        25062\n",
      "SamSam       19657\n",
      "WannaCry     16110\n",
      "JigSaw       13712\n",
      "Flyper       12014\n",
      "DMALocker    11360\n",
      "APT           9730\n",
      "CryptXXX      9335\n",
      "Razy          7862\n",
      "Globe         7373\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "expected_cols = [\"Time\",\"Protocol\",\"Flag\",\"Family\",\"Clusters\",\"SeedAddress\",\"ExpAddress\",\n",
    "                 \"BTC\",\"USD\",\"Netflow_Bytes\",\"IPaddress\",\"Threats\",\"Port\",\"Prediction\"]\n",
    "if len(df_behav.columns) == 14 and set(expected_cols) != set(df_behav.columns):\n",
    "    df_behav.columns = expected_cols\n",
    "    print(\"Renamed behavioral columns to expected schema.\")\n",
    "\n",
    "for col in [\"Prediction\",\"Family\"]:\n",
    "    if col in df_behav.columns:\n",
    "        print(f\"\\nBEHAV {col} distribution:\")\n",
    "        print(df_behav[col].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "786b72e9-2390-4024-b34b-db07716d8ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled missing values (basic).\n"
     ]
    }
   ],
   "source": [
    "num_s = df_static.select_dtypes(include=[np.number]).columns\n",
    "cat_s = df_static.select_dtypes(exclude=[np.number]).columns\n",
    "df_static[num_s] = df_static[num_s].fillna(df_static[num_s].median())\n",
    "for c in cat_s:\n",
    "    if df_static[c].isna().any():\n",
    "        df_static[c] = df_static[c].fillna(df_static[c].mode().iloc[0])\n",
    "\n",
    "num_b = df_behav.select_dtypes(include=[np.number]).columns\n",
    "cat_b = df_behav.select_dtypes(exclude=[np.number]).columns\n",
    "df_behav[num_b] = df_behav[num_b].fillna(df_behav[num_b].median())\n",
    "for c in cat_b:\n",
    "    if df_behav[c].isna().any():\n",
    "        df_behav[c] = df_behav[c].fillna(df_behav[c].mode().iloc[0])\n",
    "\n",
    "print(\"Filled missing values (basic).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28321ff6-81e1-4463-ab49-f6435b6075bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== STATIC ==\n",
      "High zero-share numeric columns:\n",
      " BitcoinAddresses      0.981996\n",
      "MinorLinkerVersion    0.686805\n",
      "ExportSize            0.672705\n",
      "ExportRVA             0.672289\n",
      "MajorImageVersion     0.586029\n",
      "DebugRVA              0.584924\n",
      "DebugSize             0.584476\n",
      "Benign                0.566008\n",
      "dtype: float64\n",
      "\n",
      "Top skewed columns:\n",
      " DebugSize            249.969998\n",
      "ResourceSize         244.030184\n",
      "DebugRVA              75.433134\n",
      "ExportSize            71.285291\n",
      "ExportRVA             54.482714\n",
      "MajorOSVersion        32.913324\n",
      "IatVRA                31.647211\n",
      "MajorImageVersion     23.132917\n",
      "dtype: float64\n",
      "\n",
      "== BEHAV ==\n",
      "High zero-share numeric columns:\n",
      " Time           0.006166\n",
      "Protocol       0.000000\n",
      "Flag           0.000000\n",
      "Family         0.000000\n",
      "Clusters       0.000000\n",
      "SeedAddress    0.000000\n",
      "ExpAddress     0.000000\n",
      "BTC            0.000000\n",
      "dtype: float64\n",
      "\n",
      "Top skewed columns:\n",
      " BTC              11.936409\n",
      "USD               3.231839\n",
      "Clusters          2.234392\n",
      "Netflow_Bytes     1.573688\n",
      "Time              0.703906\n",
      "Port              0.274006\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def sanity(df, name):\n",
    "    print(f\"\\n== {name} ==\")\n",
    "    zero_share = (df == 0).mean(numeric_only=True).sort_values(ascending=False).head(8)\n",
    "    print(\"High zero-share numeric columns:\\n\", zero_share)\n",
    "    skew = df.select_dtypes(include=[np.number]).skew().sort_values(ascending=False).head(8)\n",
    "    print(\"\\nTop skewed columns:\\n\", skew)\n",
    "\n",
    "sanity(df_static, \"STATIC\")\n",
    "sanity(df_behav,  \"BEHAV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65fe784b-3823-4418-91b6-4cf0cd295293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('C:/Users/richa/OneDrive/Documents/FYP2/data_processed/static_baseline.parquet'),\n",
       " WindowsPath('C:/Users/richa/OneDrive/Documents/FYP2/data_processed/behav_baseline.parquet'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_static = PROJECT / \"data_processed\" / \"static_baseline.parquet\"\n",
    "out_behav  = PROJECT / \"data_processed\" / \"behav_baseline.parquet\"\n",
    "out_static.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_static.to_parquet(out_static, index=False)\n",
    "df_behav.to_parquet(out_behav, index=False)\n",
    "out_static, out_behav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4649bb34-a2f5-4085-80f1-cf4b95d0a4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\richa\\onedrive\\documents\\fyp2\\.venv\\lib\\site-packages (3.0.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\richa\\onedrive\\documents\\fyp2\\.venv\\lib\\site-packages (from xgboost) (2.3.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\richa\\onedrive\\documents\\fyp2\\.venv\\lib\\site-packages (from xgboost) (1.16.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0048d3b2-890e-4d25-995c-fb5c39abccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "drop_static = [\"BitcoinAddresses\",\"ExportRVA\",\"ExportSize\",\"DebugRVA\",\"DebugSize\",\"FileName\",\"md5Hash\"]\n",
    "drop_static = [c for c in drop_static if c in df_static.columns]\n",
    "X_s = df_static.drop(columns=drop_static + [\"Benign\"])\n",
    "y_s = df_static[\"Benign\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba98fdab-07e7-4b6d-a666-accfcc70082f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STATIC RF ===\n",
      "[[7046   27]\n",
      " [  32 5392]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.995     0.996     0.996      7073\n",
      "           1      0.995     0.994     0.995      5424\n",
      "\n",
      "    accuracy                          0.995     12497\n",
      "   macro avg      0.995     0.995     0.995     12497\n",
      "weighted avg      0.995     0.995     0.995     12497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "drop_static = [\"BitcoinAddresses\",\"ExportRVA\",\"ExportSize\",\"DebugRVA\",\"DebugSize\",\"FileName\",\"md5Hash\"]\n",
    "drop_static = [c for c in drop_static if c in df_static.columns]\n",
    "X_s = df_static.drop(columns=drop_static + [\"Benign\"])\n",
    "y_s = df_static[\"Benign\"]\n",
    "\n",
    "# log1p transform some skewed cols if present\n",
    "for c in [\"ResourceSize\",\"IatVRA\",\"MajorImageVersion\",\"MajorOSVersion\"]:\n",
    "    if c in X_s.columns:\n",
    "        X_s[c] = np.log1p(X_s[c].clip(lower=0))\n",
    "\n",
    "# scale\n",
    "scaler_s = StandardScaler()\n",
    "X_s_scaled = scaler_s.fit_transform(X_s.select_dtypes(np.number))\n",
    "\n",
    "Xtr_s, Xte_s, ytr_s, yte_s = train_test_split(X_s_scaled, y_s, test_size=0.2, stratify=y_s, random_state=42)\n",
    "\n",
    "rf_s = RandomForestClassifier(n_estimators=300, class_weight=\"balanced\", random_state=42, n_jobs=-1)\n",
    "rf_s.fit(Xtr_s, ytr_s)\n",
    "pred_s = rf_s.predict(Xte_s)\n",
    "print(\"=== STATIC RF ===\")\n",
    "print(confusion_matrix(yte_s, pred_s))\n",
    "print(classification_report(yte_s, pred_s, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa340568-6af9-4353-914d-6209814401d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STATIC RF ===\n",
      "[[7046   27]\n",
      " [  32 5392]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.995     0.996     0.996      7073\n",
      "           1      0.995     0.994     0.995      5424\n",
      "\n",
      "    accuracy                          0.995     12497\n",
      "   macro avg      0.995     0.995     0.995     12497\n",
      "weighted avg      0.995     0.995     0.995     12497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === STATIC: clean + feature set + RF baseline ===\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Columns to drop if present (near-constant or identifiers)\n",
    "drop_static = [\"BitcoinAddresses\",\"ExportRVA\",\"ExportSize\",\"DebugRVA\",\"DebugSize\",\"FileName\",\"md5Hash\"]\n",
    "drop_static = [c for c in drop_static if c in df_static.columns]\n",
    "\n",
    "# Features/target\n",
    "X_s = df_static.drop(columns=drop_static + [\"Benign\"])\n",
    "y_s = df_static[\"Benign\"]\n",
    "\n",
    "# Log-transform some skewed positive columns (safe with clip lower=0)\n",
    "for c in [\"ResourceSize\",\"IatVRA\",\"MajorImageVersion\",\"MajorOSVersion\"]:\n",
    "    if c in X_s.columns:\n",
    "        X_s[c] = np.log1p(X_s[c].clip(lower=0))\n",
    "\n",
    "# Scale (keep as DataFrame with original column names for interpretability)\n",
    "scaler_s = StandardScaler()\n",
    "X_s_scaled_arr = scaler_s.fit_transform(X_s)\n",
    "X_s_scaled = pd.DataFrame(X_s_scaled_arr, columns=X_s.columns)\n",
    "\n",
    "# Split\n",
    "Xtr_s, Xte_s, ytr_s, yte_s = train_test_split(\n",
    "    X_s_scaled, y_s, test_size=0.2, stratify=y_s, random_state=42\n",
    ")\n",
    "\n",
    "# Model\n",
    "rf_s = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_s.fit(Xtr_s, ytr_s)\n",
    "pred_s = rf_s.predict(Xte_s)\n",
    "\n",
    "print(\"=== STATIC RF ===\")\n",
    "print(confusion_matrix(yte_s, pred_s))\n",
    "print(classification_report(yte_s, pred_s, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abf7f03e-ac7d-417f-bb7a-19d4cf3917ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:Protocol: object, Flag: object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\data.py:407\u001b[39m, in \u001b[36mpandas_feature_info\u001b[39m\u001b[34m(data, meta, feature_names, feature_types, enable_categorical)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     new_feature_types.append(\u001b[43m_pandas_dtype_mapper\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[31mKeyError\u001b[39m: 'object'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     50\u001b[39m Xtr_b, Xte_b, ytr_b, yte_b = train_test_split(\n\u001b[32m     51\u001b[39m     X_b, y_b, test_size=\u001b[32m0.2\u001b[39m, stratify=y_b, random_state=\u001b[32m42\u001b[39m\n\u001b[32m     52\u001b[39m )\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Fit with eval set so early stopping can monitor validation performance\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[43mpipe_b\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mXtr_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytr_b\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclf__eval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXte_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myte_b\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclf__verbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     59\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m pred_b = pipe_b.predict(Xte_b)\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== BEHAV XGB (WannaCry vs Not) ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:663\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    657\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    658\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    659\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    660\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    661\u001b[39m             all_params=params,\n\u001b[32m    662\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\sklearn.py:1664\u001b[39m, in \u001b[36mXGBClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1659\u001b[39m     params[\u001b[33m\"\u001b[39m\u001b[33mnum_class\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.n_classes_\n\u001b[32m   1661\u001b[39m model, metric, params, feature_weights = \u001b[38;5;28mself\u001b[39m._configure_fit(\n\u001b[32m   1662\u001b[39m     xgb_model, params, feature_weights\n\u001b[32m   1663\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1664\u001b[39m train_dmatrix, evals = \u001b[43m_wrap_evaluation_matrices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1665\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1666\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1667\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1670\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1673\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1674\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1676\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_group\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1677\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_qid\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_dmatrix\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1679\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1680\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1681\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1683\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = train(\n\u001b[32m   1684\u001b[39m     params,\n\u001b[32m   1685\u001b[39m     train_dmatrix,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1694\u001b[39m     callbacks=\u001b[38;5;28mself\u001b[39m.callbacks,\n\u001b[32m   1695\u001b[39m )\n\u001b[32m   1697\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.objective):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\sklearn.py:679\u001b[39m, in \u001b[36m_wrap_evaluation_matrices\u001b[39m\u001b[34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[39m\n\u001b[32m    677\u001b[39m         evals.append(train_dmatrix)\n\u001b[32m    678\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m679\u001b[39m         m = \u001b[43mcreate_dmatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m            \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_group\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m            \u001b[49m\u001b[43mqid\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_qid\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m            \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m            \u001b[49m\u001b[43mref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    691\u001b[39m         evals.append(m)\n\u001b[32m    692\u001b[39m nevals = \u001b[38;5;28mlen\u001b[39m(evals)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\sklearn.py:1137\u001b[39m, in \u001b[36mXGBModel._create_dmatrix\u001b[39m\u001b[34m(self, ref, **kwargs)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _can_use_qdm(\u001b[38;5;28mself\u001b[39m.tree_method, \u001b[38;5;28mself\u001b[39m.device) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.booster != \u001b[33m\"\u001b[39m\u001b[33mgblinear\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1136\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQuantileDMatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnthread\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bin\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_bin\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1140\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[39;00m\n\u001b[32m   1141\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\core.py:1614\u001b[39m, in \u001b[36mQuantileDMatrix.__init__\u001b[39m\u001b[34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, max_quantile_batches, data_split_mode)\u001b[39m\n\u001b[32m   1594\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m   1595\u001b[39m         info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1596\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m   (...)\u001b[39m\u001b[32m   1607\u001b[39m         )\n\u001b[32m   1608\u001b[39m     ):\n\u001b[32m   1609\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1610\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mIf data iterator is used as input, data like label should be \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1611\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mspecified as batch argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1612\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1614\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1615\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1616\u001b[39m \u001b[43m    \u001b[49m\u001b[43mref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1617\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1618\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1619\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1620\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1621\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1627\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_quantile_blocks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_quantile_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1629\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\core.py:1678\u001b[39m, in \u001b[36mQuantileDMatrix._init\u001b[39m\u001b[34m(self, data, ref, enable_categorical, max_quantile_blocks, **meta)\u001b[39m\n\u001b[32m   1663\u001b[39m config = make_jcargs(\n\u001b[32m   1664\u001b[39m     nthread=\u001b[38;5;28mself\u001b[39m.nthread,\n\u001b[32m   1665\u001b[39m     missing=\u001b[38;5;28mself\u001b[39m.missing,\n\u001b[32m   1666\u001b[39m     max_bin=\u001b[38;5;28mself\u001b[39m.max_bin,\n\u001b[32m   1667\u001b[39m     max_quantile_blocks=max_quantile_blocks,\n\u001b[32m   1668\u001b[39m )\n\u001b[32m   1669\u001b[39m ret = _LIB.XGQuantileDMatrixCreateFromCallback(\n\u001b[32m   1670\u001b[39m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1671\u001b[39m     it.proxy.handle,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1676\u001b[39m     ctypes.byref(handle),\n\u001b[32m   1677\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1678\u001b[39m \u001b[43mit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1679\u001b[39m \u001b[38;5;66;03m# delay check_call to throw intermediate exception first\u001b[39;00m\n\u001b[32m   1680\u001b[39m _check_call(ret)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\core.py:572\u001b[39m, in \u001b[36mDataIter.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    570\u001b[39m exc = \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    571\u001b[39m \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m572\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\core.py:553\u001b[39m, in \u001b[36mDataIter._handle_exception\u001b[39m\u001b[34m(self, fn, dft_ret)\u001b[39m\n\u001b[32m    550\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dft_ret\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    555\u001b[39m     \u001b[38;5;66;03m# Defer the exception in order to return 0 and stop the iteration.\u001b[39;00m\n\u001b[32m    556\u001b[39m     \u001b[38;5;66;03m# Exception inside a ctype callback function has no effect except\u001b[39;00m\n\u001b[32m    557\u001b[39m     \u001b[38;5;66;03m# for printing to stderr (doesn't stop the execution).\u001b[39;00m\n\u001b[32m    558\u001b[39m     tb = sys.exc_info()[\u001b[32m2\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\core.py:640\u001b[39m, in \u001b[36mDataIter._next_wrapper.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    638\u001b[39m     \u001b[38;5;28mself\u001b[39m._temporary_data = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    639\u001b[39m \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handle_exception(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m), \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\data.py:1654\u001b[39m, in \u001b[36mSingleBatchInternalIter.next\u001b[39m\u001b[34m(self, input_data)\u001b[39m\n\u001b[32m   1652\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1653\u001b[39m \u001b[38;5;28mself\u001b[39m.it += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1654\u001b[39m \u001b[43minput_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1655\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\core.py:620\u001b[39m, in \u001b[36mDataIter._next_wrapper.<locals>.input_data\u001b[39m\u001b[34m(data, feature_names, feature_types, **kwargs)\u001b[39m\n\u001b[32m    618\u001b[39m     new, cat_codes, feature_names, feature_types = \u001b[38;5;28mself\u001b[39m._temporary_data\n\u001b[32m    619\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m     new, cat_codes, feature_names, feature_types = \u001b[43m_proxy_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_enable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[38;5;66;03m# Stage the data, meta info are copied inside C++ MetaInfo.\u001b[39;00m\n\u001b[32m    627\u001b[39m \u001b[38;5;28mself\u001b[39m._temporary_data = (new, cat_codes, feature_names, feature_types)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\data.py:1707\u001b[39m, in \u001b[36m_proxy_transform\u001b[39m\u001b[34m(data, feature_names, feature_types, enable_categorical)\u001b[39m\n\u001b[32m   1705\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df_pa, \u001b[38;5;28;01mNone\u001b[39;00m, feature_names, feature_types\n\u001b[32m   1706\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_pandas_df(data):\n\u001b[32m-> \u001b[39m\u001b[32m1707\u001b[39m     df, feature_names, feature_types = \u001b[43m_transform_pandas_df\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1708\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_types\u001b[49m\n\u001b[32m   1709\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1710\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df, \u001b[38;5;28;01mNone\u001b[39;00m, feature_names, feature_types\n\u001b[32m   1711\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mValue type is not supported for data iterator:\u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(data)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\data.py:640\u001b[39m, in \u001b[36m_transform_pandas_df\u001b[39m\u001b[34m(data, enable_categorical, feature_names, feature_types, meta)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data.columns) > \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m meta \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _matrix_meta:\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataFrame for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeta\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cannot have multiple columns\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m feature_names, feature_types = \u001b[43mpandas_feature_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_categorical\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    644\u001b[39m arrays = pandas_transform_data(data)\n\u001b[32m    645\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m PandasTransformed(arrays), feature_names, feature_types\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\data.py:409\u001b[39m, in \u001b[36mpandas_feature_info\u001b[39m\u001b[34m(data, meta, feature_names, feature_types, enable_categorical)\u001b[39m\n\u001b[32m    407\u001b[39m             new_feature_types.append(_pandas_dtype_mapper[dtype.name])\n\u001b[32m    408\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m             \u001b[43m_invalid_dataframe_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m feature_types \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m meta \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    412\u001b[39m     feature_types = new_feature_types\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\data.py:372\u001b[39m, in \u001b[36m_invalid_dataframe_dtype\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    370\u001b[39m type_err = \u001b[33m\"\u001b[39m\u001b[33mDataFrame.dtypes for data must be int, float, bool or category.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    371\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtype_err\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_ENABLE_CAT_ERR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[31mValueError\u001b[39m: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:Protocol: object, Flag: object"
     ]
    }
   ],
   "source": [
    "# === BEHAVIORAL: WannaCry vs Not (XGBoost + early stopping) ===\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Build target\n",
    "df_b = df_behav.copy()\n",
    "df_b[\"is_wannacry\"] = (df_b[\"Family\"] == \"WannaCry\").astype(int)\n",
    "\n",
    "# Feature sets\n",
    "num_b = [\"Time\",\"Netflow_Bytes\",\"BTC\",\"USD\",\"Port\"]\n",
    "cat_b = [\"Protocol\",\"Flag\"]  # keep minimal to start; add more later if needed\n",
    "\n",
    "# Log-transform skewed numerics\n",
    "for c in [\"BTC\",\"USD\",\"Netflow_Bytes\"]:\n",
    "    if c in df_b.columns:\n",
    "        df_b[c] = np.log1p(df_b[c].clip(lower=0))\n",
    "\n",
    "X_b = df_b[num_b + cat_b]\n",
    "y_b = df_b[\"is_wannacry\"]\n",
    "\n",
    "# Preprocess: passthrough numerics, one-hot categoricals\n",
    "pre_b = ColumnTransformer([\n",
    "    (\"num\",\"passthrough\", num_b),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_b)\n",
    "])\n",
    "\n",
    "# XGB with early stopping (needs eval_set during .fit)\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.08,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    eval_metric=\"logloss\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=20\n",
    ")\n",
    "\n",
    "pipe_b = Pipeline([\n",
    "    (\"prep\", pre_b),\n",
    "    (\"clf\", xgb)\n",
    "])\n",
    "\n",
    "Xtr_b, Xte_b, ytr_b, yte_b = train_test_split(\n",
    "    X_b, y_b, test_size=0.2, stratify=y_b, random_state=42\n",
    ")\n",
    "\n",
    "# Fit with eval set so early stopping can monitor validation performance\n",
    "pipe_b.fit(\n",
    "    Xtr_b, ytr_b,\n",
    "    clf__eval_set=[(Xte_b, yte_b)],\n",
    "    clf__verbose=False\n",
    ")\n",
    "\n",
    "pred_b = pipe_b.predict(Xte_b)\n",
    "\n",
    "print(\"=== BEHAV XGB (WannaCry vs Not) ===\")\n",
    "print(confusion_matrix(yte_b, pred_b))\n",
    "print(classification_report(yte_b, pred_b, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c545a812-6177-49f1-ba0c-10a9766f256b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEHAV XGB (WannaCry vs Not) ===\n",
      "[[26587     0]\n",
      " [ 3193    29]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.893     1.000     0.943     26587\n",
      "           1      1.000     0.009     0.018      3222\n",
      "\n",
      "    accuracy                          0.893     29809\n",
      "   macro avg      0.946     0.505     0.481     29809\n",
      "weighted avg      0.904     0.893     0.843     29809\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# --- build labels and features as before ---\n",
    "df_b = df_behav.copy()\n",
    "df_b[\"is_wannacry\"] = (df_b[\"Family\"] == \"WannaCry\").astype(int)\n",
    "\n",
    "num_b = [\"Time\",\"Netflow_Bytes\",\"BTC\",\"USD\",\"Port\"]\n",
    "cat_b = [\"Protocol\",\"Flag\"]\n",
    "\n",
    "for c in [\"BTC\",\"USD\",\"Netflow_Bytes\"]:\n",
    "    if c in df_b.columns:\n",
    "        df_b[c] = np.log1p(df_b[c].clip(lower=0))\n",
    "\n",
    "X_b = df_b[num_b + cat_b]\n",
    "y_b = df_b[\"is_wannacry\"]\n",
    "\n",
    "pre_b = ColumnTransformer([\n",
    "    (\"num\",\"passthrough\", num_b),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_b)\n",
    "])\n",
    "\n",
    "Xtr_b, Xte_b, ytr_b, yte_b = train_test_split(\n",
    "    X_b, y_b, test_size=0.2, stratify=y_b, random_state=42\n",
    ")\n",
    "\n",
    "# --- fit the preprocessor and transform both splits ---\n",
    "pre_b.fit(Xtr_b, ytr_b)\n",
    "Xtr_t = pre_b.transform(Xtr_b)   # numpy / sparse matrix (OK for XGB)\n",
    "Xte_t = pre_b.transform(Xte_b)\n",
    "\n",
    "# --- XGBoost with early stopping ---\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.08,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    eval_metric=\"logloss\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=20\n",
    ")\n",
    "\n",
    "xgb.fit(\n",
    "    Xtr_t, ytr_b,\n",
    "    eval_set=[(Xte_t, yte_b)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# --- wrap back into a pipeline object for convenient predict/save ---\n",
    "pipe_b = Pipeline([(\"prep\", pre_b), (\"clf\", xgb)])\n",
    "\n",
    "pred_b = pipe_b.predict(Xte_b)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"=== BEHAV XGB (WannaCry vs Not) ===\")\n",
    "print(confusion_matrix(yte_b, pred_b))\n",
    "print(classification_report(yte_b, pred_b, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ba38604-eb95-4b33-93ec-bc8d73ced402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: [WindowsPath('models/behav_xgb_pipeline.joblib'), WindowsPath('models/static_feature_names.joblib'), WindowsPath('models/static_rf.joblib'), WindowsPath('models/static_scaler.joblib')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_DIR = Path(\"models\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "joblib.dump(rf_s, MODEL_DIR / \"static_rf.joblib\")\n",
    "joblib.dump(scaler_s, MODEL_DIR / \"static_scaler.joblib\")\n",
    "joblib.dump(X_s.columns.tolist(), MODEL_DIR / \"static_feature_names.joblib\")\n",
    "\n",
    "# Behavioral pipeline (includes preprocessing + model)\n",
    "joblib.dump(pipe_b, MODEL_DIR / \"behav_xgb_pipeline.joblib\")\n",
    "\n",
    "print(\"Saved:\", list(MODEL_DIR.iterdir()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6a32e4b-2a96-47af-a886-fa9ea731f170",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['SeddAddress'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df_b.columns:\n\u001b[32m     22\u001b[39m         df_b[c] = np.log1p(df_b[c].clip(lower=\u001b[32m0\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m X_b = \u001b[43mdf_b\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnum_b\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_card_cat\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_cat\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     25\u001b[39m y_b = df_b[\u001b[33m\"\u001b[39m\u001b[33mis_wannacry\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     27\u001b[39m Xtr_b, Xte_b, ytr_b, yte_b = train_test_split(\n\u001b[32m     28\u001b[39m     X_b, y_b, test_size=\u001b[32m0.2\u001b[39m, stratify=y_b, random_state=\u001b[32m42\u001b[39m\n\u001b[32m     29\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4112\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4115\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['SeddAddress'] not in index\""
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df_b = df_behav.copy()\n",
    "df_b[\"is_wannacry\"] = (df_b[\"Family\"] == \"WannaCry\").astype(int)\n",
    "\n",
    "\n",
    "num_b = [\"Time\",\"Netflow_Bytes\",\"BTC\",\"USD\",\"Port\",\"Clusters\"]\n",
    "low_card_cat = [\"Protocol\",\"Flag\",\"Threats\"]      \n",
    "hash_cat = [\"SeddAddress\",\"ExpAddress\"]           \n",
    "\n",
    "\n",
    "for c in [\"BTC\",\"USD\",\"Netflow_Bytes\"]:\n",
    "    if c in df_b.columns:\n",
    "        df_b[c] = np.log1p(df_b[c].clip(lower=0))\n",
    "\n",
    "X_b = df_b[num_b + low_card_cat + hash_cat]\n",
    "y_b = df_b[\"is_wannacry\"]\n",
    "\n",
    "Xtr_b, Xte_b, ytr_b, yte_b = train_test_split(\n",
    "    X_b, y_b, test_size=0.2, stratify=y_b, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "neg = (ytr_b == 0).sum()\n",
    "pos = (ytr_b == 1).sum()\n",
    "spw = neg / max(1, pos)\n",
    "\n",
    "\n",
    "pre_b = ColumnTransformer([\n",
    "    (\"num\", \"passthrough\", num_b),\n",
    "    (\"lowcat\", OneHotEncoder(handle_unknown=\"ignore\", sparse=True), low_card_cat),\n",
    "    (\"hash\", FeatureHasher(n_features=2**12, input_type=\"string\"), hash_cat),\n",
    "], sparse_threshold=1.0)   # keep sparse, XGB handles it well\n",
    "\n",
    "\n",
    "pre_b.fit(Xtr_b, ytr_b)\n",
    "Xtr_t = pre_b.transform(Xtr_b)\n",
    "Xte_t = pre_b.transform(Xte_b)\n",
    "\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    eval_metric=\"aucpr\",          # focus on positive class\n",
    "    scale_pos_weight=spw,         # handle imbalance\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=30\n",
    ")\n",
    "\n",
    "xgb.fit(\n",
    "    Xtr_t, ytr_b,\n",
    "    eval_set=[(Xte_t, yte_b)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe_b = Pipeline([(\"prep\", pre_b), (\"clf\", xgb)])\n",
    "\n",
    "\n",
    "proba = pipe_b.predict_proba(Xte_b)[:,1]\n",
    "prec, rec, thr = precision_recall_curve(yte_b, proba)\n",
    "\n",
    "\n",
    "f1 = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "best_idx = np.nanargmax(f1)\n",
    "best_thr = thr[best_idx] if best_idx < len(thr) else 0.5\n",
    "\n",
    "print(f\"Chosen threshold (max F1): {best_thr:.3f}  |  PR-AUC-guided\")\n",
    "\n",
    "pred_b = (proba >= best_thr).astype(int)\n",
    "print(\"=== BEHAV XGB (WannaCry vs Not) with tuned threshold ===\")\n",
    "print(confusion_matrix(yte_b, pred_b))\n",
    "print(classification_report(yte_b, pred_b, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79081250-98eb-4be3-aa63-28661a550f9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['SeddAddress'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df_b.columns:\n\u001b[32m     22\u001b[39m         df_b[c] = np.log1p(df_b[c].clip(lower=\u001b[32m0\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m X_b = \u001b[43mdf_b\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnum_b\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_card_cat\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_cat\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     25\u001b[39m y_b = df_b[\u001b[33m\"\u001b[39m\u001b[33mis_wannacry\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     27\u001b[39m Xtr_b, Xte_b, ytr_b, yte_b = train_test_split(\n\u001b[32m     28\u001b[39m     X_b, y_b, test_size=\u001b[32m0.2\u001b[39m, stratify=y_b, random_state=\u001b[32m42\u001b[39m\n\u001b[32m     29\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4112\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4115\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['SeddAddress'] not in index\""
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df_b = df_behav.copy()\n",
    "df_b[\"is_wannacry\"] = (df_b[\"Family\"] == \"WannaCry\").astype(int)\n",
    "\n",
    "\n",
    "num_b = [\"Time\",\"Netflow_Bytes\",\"BTC\",\"USD\",\"Port\",\"Clusters\"]\n",
    "low_card_cat = [\"Protocol\",\"Flag\",\"Threats\"]      \n",
    "hash_cat = [\"SeddAddress\",\"ExpAddress\"]           \n",
    "\n",
    "\n",
    "for c in [\"BTC\",\"USD\",\"Netflow_Bytes\"]:\n",
    "    if c in df_b.columns:\n",
    "        df_b[c] = np.log1p(df_b[c].clip(lower=0))\n",
    "\n",
    "X_b = df_b[num_b + low_card_cat + hash_cat]\n",
    "y_b = df_b[\"is_wannacry\"]\n",
    "\n",
    "Xtr_b, Xte_b, ytr_b, yte_b = train_test_split(\n",
    "    X_b, y_b, test_size=0.2, stratify=y_b, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "neg = (ytr_b == 0).sum()\n",
    "pos = (ytr_b == 1).sum()\n",
    "spw = neg / max(1, pos)\n",
    "\n",
    "\n",
    "pre_b = ColumnTransformer([\n",
    "    (\"num\", \"passthrough\", num_b),\n",
    "    (\"lowcat\", OneHotEncoder(handle_unknown=\"ignore\", sparse=True), low_card_cat),\n",
    "    (\"hash\", FeatureHasher(n_features=2**12, input_type=\"string\"), hash_cat),\n",
    "], sparse_threshold=1.0)   # keep sparse, XGB handles it well\n",
    "\n",
    "\n",
    "pre_b.fit(Xtr_b, ytr_b)\n",
    "Xtr_t = pre_b.transform(Xtr_b)\n",
    "Xte_t = pre_b.transform(Xte_b)\n",
    "\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    eval_metric=\"aucpr\",          # focus on positive class\n",
    "    scale_pos_weight=spw,         # handle imbalance\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=30\n",
    ")\n",
    "\n",
    "xgb.fit(\n",
    "    Xtr_t, ytr_b,\n",
    "    eval_set=[(Xte_t, yte_b)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe_b = Pipeline([(\"prep\", pre_b), (\"clf\", xgb)])\n",
    "\n",
    "\n",
    "proba = pipe_b.predict_proba(Xte_b)[:,1]\n",
    "prec, rec, thr = precision_recall_curve(yte_b, proba)\n",
    "\n",
    "\n",
    "f1 = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "best_idx = np.nanargmax(f1)\n",
    "best_thr = thr[best_idx] if best_idx < len(thr) else 0.5\n",
    "\n",
    "print(f\"Chosen threshold (max F1): {best_thr:.3f}  |  PR-AUC-guided\")\n",
    "\n",
    "pred_b = (proba >= best_thr).astype(int)\n",
    "print(\"=== BEHAV XGB (WannaCry vs Not) with tuned threshold ===\")\n",
    "print(confusion_matrix(yte_b, pred_b))\n",
    "print(classification_report(yte_b, pred_b, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4dc19243-348e-416b-8dae-3379e78ab2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time', 'Protocol', 'Flag', 'Family', 'Clusters', 'SeedAddress', 'ExpAddress', 'BTC', 'USD', 'Netflow_Bytes', 'IPaddress', 'Threats', 'Port', 'Prediction', 'is_wannacry']\n"
     ]
    }
   ],
   "source": [
    "print(df_b.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbb97d47-4792-4f76-8a43-36ec448b09b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     34\u001b[39m spw = neg / \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, pos)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# ---------- Preprocess ----------\u001b[39;00m\n\u001b[32m     37\u001b[39m pre_b = ColumnTransformer([\n\u001b[32m     38\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mnum\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m, num_b),\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mlowcat\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mOneHotEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mignore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m, low_card_cat),\n\u001b[32m     40\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mhash\u001b[39m\u001b[33m\"\u001b[39m, FeatureHasher(n_features=\u001b[32m2\u001b[39m**\u001b[32m12\u001b[39m, input_type=\u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m), hash_cat),\n\u001b[32m     41\u001b[39m ], sparse_threshold=\u001b[32m1.0\u001b[39m)   \u001b[38;5;66;03m# keep sparse, XGB handles it well\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# fit preprocessor and transform sets for early stopping\u001b[39;00m\n\u001b[32m     44\u001b[39m pre_b.fit(Xtr_b, ytr_b)\n",
      "\u001b[31mTypeError\u001b[39m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# ---------- Build label ----------\n",
    "df_b = df_behav.copy()\n",
    "df_b[\"is_wannacry\"] = (df_b[\"Family\"] == \"WannaCry\").astype(int)\n",
    "\n",
    "# ---------- Columns ----------\n",
    "num_b = [\"Time\",\"Netflow_Bytes\",\"BTC\",\"USD\",\"Port\",\"Clusters\"]\n",
    "low_card_cat = [\"Protocol\",\"Flag\",\"Threats\"]      # one-hot\n",
    "hash_cat = [\"SeedAddress\",\"ExpAddress\"]           # hashed to keep dims small\n",
    "\n",
    "# log-transform skewed numerics\n",
    "for c in [\"BTC\",\"USD\",\"Netflow_Bytes\"]:\n",
    "    if c in df_b.columns:\n",
    "        df_b[c] = np.log1p(df_b[c].clip(lower=0))\n",
    "\n",
    "X_b = df_b[num_b + low_card_cat + hash_cat]\n",
    "y_b = df_b[\"is_wannacry\"]\n",
    "\n",
    "Xtr_b, Xte_b, ytr_b, yte_b = train_test_split(\n",
    "    X_b, y_b, test_size=0.2, stratify=y_b, random_state=42\n",
    ")\n",
    "\n",
    "# scale_pos_weight = negatives / positives (compute on train only)\n",
    "neg = (ytr_b == 0).sum()\n",
    "pos = (ytr_b == 1).sum()\n",
    "spw = neg / max(1, pos)\n",
    "\n",
    "# ---------- Preprocess ----------\n",
    "pre_b = ColumnTransformer([\n",
    "    (\"num\", \"passthrough\", num_b),\n",
    "    (\"lowcat\", OneHotEncoder(handle_unknown=\"ignore\", sparse=True), low_card_cat),\n",
    "    (\"hash\", FeatureHasher(n_features=2**12, input_type=\"string\"), hash_cat),\n",
    "], sparse_threshold=1.0)   # keep sparse, XGB handles it well\n",
    "\n",
    "# fit preprocessor and transform sets for early stopping\n",
    "pre_b.fit(Xtr_b, ytr_b)\n",
    "Xtr_t = pre_b.transform(Xtr_b)\n",
    "Xte_t = pre_b.transform(Xte_b)\n",
    "\n",
    "# ---------- XGBoost ----------\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    eval_metric=\"aucpr\",          # focus on positive class\n",
    "    scale_pos_weight=spw,         # handle imbalance\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=30\n",
    ")\n",
    "\n",
    "xgb.fit(\n",
    "    Xtr_t, ytr_b,\n",
    "    eval_set=[(Xte_t, yte_b)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Wrap back into a pipeline for later .predict/.predict_proba and saving\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe_b = Pipeline([(\"prep\", pre_b), (\"clf\", xgb)])\n",
    "\n",
    "# ---------- Threshold tuning ----------\n",
    "proba = pipe_b.predict_proba(Xte_b)[:,1]\n",
    "prec, rec, thr = precision_recall_curve(yte_b, proba)\n",
    "\n",
    "# choose threshold that maximizes F1 on the validation set\n",
    "f1 = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "best_idx = np.nanargmax(f1)\n",
    "best_thr = thr[best_idx] if best_idx < len(thr) else 0.5\n",
    "\n",
    "print(f\"Chosen threshold (max F1): {best_thr:.3f}  |  PR-AUC-guided\")\n",
    "\n",
    "pred_b = (proba >= best_thr).astype(int)\n",
    "print(\"=== BEHAV XGB (WannaCry vs Not) with tuned threshold ===\")\n",
    "print(confusion_matrix(yte_b, pred_b))\n",
    "print(classification_report(yte_b, pred_b, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877d584d-13a5-4971-898c-5a1bd2695bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6281978-1f11-4d0a-80cf-134de8245fb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['SeddAddress'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df_b.columns:\n\u001b[32m     22\u001b[39m         df_b[c] = np.log1p(df_b[c].clip(lower=\u001b[32m0\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m X_b = \u001b[43mdf_b\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnum_b\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_card_cat\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_cat\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     25\u001b[39m y_b = df_b[\u001b[33m\"\u001b[39m\u001b[33mis_wannacry\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     27\u001b[39m Xtr_b, Xte_b, ytr_b, yte_b = train_test_split(\n\u001b[32m     28\u001b[39m     X_b, y_b, test_size=\u001b[32m0.2\u001b[39m, stratify=y_b, random_state=\u001b[32m42\u001b[39m\n\u001b[32m     29\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4112\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4115\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['SeddAddress'] not in index\""
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# ---------- Build label ----------\n",
    "df_b = df_behav.copy()\n",
    "df_b[\"is_wannacry\"] = (df_b[\"Family\"] == \"WannaCry\").astype(int)\n",
    "\n",
    "# ---------- Columns ----------\n",
    "num_b = [\"Time\",\"Netflow_Bytes\",\"BTC\",\"USD\",\"Port\",\"Clusters\"]\n",
    "low_card_cat = [\"Protocol\",\"Flag\",\"Threats\"]      # one-hot\n",
    "hash_cat = [\"SeddAddress\",\"ExpAddress\"]           # hashed to keep dims small\n",
    "\n",
    "# log-transform skewed numerics\n",
    "for c in [\"BTC\",\"USD\",\"Netflow_Bytes\"]:\n",
    "    if c in df_b.columns:\n",
    "        df_b[c] = np.log1p(df_b[c].clip(lower=0))\n",
    "\n",
    "X_b = df_b[num_b + low_card_cat + hash_cat]\n",
    "y_b = df_b[\"is_wannacry\"]\n",
    "\n",
    "Xtr_b, Xte_b, ytr_b, yte_b = train_test_split(\n",
    "    X_b, y_b, test_size=0.2, stratify=y_b, random_state=42\n",
    ")\n",
    "\n",
    "# scale_pos_weight = negatives / positives (compute on train only)\n",
    "neg = (ytr_b == 0).sum()\n",
    "pos = (ytr_b == 1).sum()\n",
    "spw = neg / max(1, pos)\n",
    "\n",
    "\n",
    "pre_b = ColumnTransformer(\n",
    "    [\n",
    "        (\"num\",  \"passthrough\", num_b),\n",
    "        (\"lowcat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), low_card_cat),\n",
    "        (\"hash\", FeatureHasher(n_features=2**12, input_type=\"string\"), hash_cat),\n",
    "    ],\n",
    "    sparse_threshold=1.0  \n",
    ")\n",
    "\n",
    "pre_b.fit(Xtr_b, ytr_b)\n",
    "Xtr_t = pre_b.transform(Xtr_b)\n",
    "Xte_t = pre_b.transform(Xte_b)\n",
    "\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    eval_metric=\"aucpr\",          # focus on positive class\n",
    "    scale_pos_weight=spw,         # handle imbalance\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=30\n",
    ")\n",
    "\n",
    "xgb.fit(\n",
    "    Xtr_t, ytr_b,\n",
    "    eval_set=[(Xte_t, yte_b)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe_b = Pipeline([(\"prep\", pre_b), (\"clf\", xgb)])\n",
    "\n",
    "\n",
    "proba = pipe_b.predict_proba(Xte_b)[:,1]\n",
    "prec, rec, thr = precision_recall_curve(yte_b, proba)\n",
    "\n",
    "\n",
    "f1 = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "best_idx = np.nanargmax(f1)\n",
    "best_thr = thr[best_idx] if best_idx < len(thr) else 0.5\n",
    "\n",
    "print(f\"Chosen threshold (max F1): {best_thr:.3f}  |  PR-AUC-guided\")\n",
    "\n",
    "pred_b = (proba >= best_thr).astype(int)\n",
    "print(\"=== BEHAV XGB (WannaCry vs Not) with tuned threshold ===\")\n",
    "print(confusion_matrix(yte_b, pred_b))\n",
    "print(classification_report(yte_b, pred_b, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be0a43df-b22a-46ad-bd6a-1555d6cdb3d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Samples can not be a single string. The input must be an iterable over iterables of strings.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     34\u001b[39m spw = neg / \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, pos)\n\u001b[32m     37\u001b[39m pre_b = ColumnTransformer(\n\u001b[32m     38\u001b[39m     [\n\u001b[32m     39\u001b[39m         (\u001b[33m\"\u001b[39m\u001b[33mnum\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m, num_b),\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m     sparse_threshold=\u001b[32m1.0\u001b[39m  \n\u001b[32m     44\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[43mpre_b\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtr_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytr_b\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m Xtr_t = pre_b.transform(Xtr_b)\n\u001b[32m     49\u001b[39m Xte_t = pre_b.transform(Xte_b)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:934\u001b[39m, in \u001b[36mColumnTransformer.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    931\u001b[39m _raise_for_params(params, \u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    932\u001b[39m \u001b[38;5;66;03m# we use fit_transform to make sure to set sparse_output_ (for which we\u001b[39;00m\n\u001b[32m    933\u001b[39m \u001b[38;5;66;03m# need the transformed data) to have consistent output type in predict\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m934\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:996\u001b[39m, in \u001b[36mColumnTransformer.fit_transform\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    993\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    994\u001b[39m     routed_params = \u001b[38;5;28mself\u001b[39m._get_empty_routing()\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_func_on_transformers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    997\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_fit_transform_one\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumn_as_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result:\n\u001b[32m   1005\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_fitted_transformers([])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:897\u001b[39m, in \u001b[36mColumnTransformer._call_func_on_transformers\u001b[39m\u001b[34m(self, X, y, func, column_as_labels, routed_params)\u001b[39m\n\u001b[32m    885\u001b[39m             extra_args = {}\n\u001b[32m    886\u001b[39m         jobs.append(\n\u001b[32m    887\u001b[39m             delayed(func)(\n\u001b[32m    888\u001b[39m                 transformer=clone(trans) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted \u001b[38;5;28;01melse\u001b[39;00m trans,\n\u001b[32m   (...)\u001b[39m\u001b[32m    894\u001b[39m             )\n\u001b[32m    895\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mExpected 2D array, got 1D array instead\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:1540\u001b[39m, in \u001b[36m_fit_transform_one\u001b[39m\u001b[34m(transformer, X, y, weight, message_clsname, message, params)\u001b[39m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[32m   1539\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1540\u001b[39m         res = \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1541\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1542\u001b[39m         res = transformer.fit(X, y, **params.get(\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, {})).transform(\n\u001b[32m   1543\u001b[39m             X, **params.get(\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m   1544\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\sklearn\\base.py:897\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    894\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, **fit_params).transform(X)\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    896\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\_hash.py:177\u001b[39m, in \u001b[36mFeatureHasher.transform\u001b[39m\u001b[34m(self, raw_X)\u001b[39m\n\u001b[32m    175\u001b[39m first_raw_X = \u001b[38;5;28mnext\u001b[39m(raw_X)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_raw_X, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    178\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSamples can not be a single string. The input must be an iterable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    179\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m over iterables of strings.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    180\u001b[39m     )\n\u001b[32m    181\u001b[39m raw_X_ = chain([first_raw_X], raw_X)\n\u001b[32m    182\u001b[39m raw_X = (((f, \u001b[32m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m raw_X_)\n",
      "\u001b[31mValueError\u001b[39m: Samples can not be a single string. The input must be an iterable over iterables of strings."
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# ---------- Build label ----------\n",
    "df_b = df_behav.copy()\n",
    "df_b[\"is_wannacry\"] = (df_b[\"Family\"] == \"WannaCry\").astype(int)\n",
    "\n",
    "# ---------- Columns ----------\n",
    "num_b = [\"Time\",\"Netflow_Bytes\",\"BTC\",\"USD\",\"Port\",\"Clusters\"]\n",
    "low_card_cat = [\"Protocol\",\"Flag\",\"Threats\"]      \n",
    "hash_cat = [\"SeedAddress\",\"ExpAddress\"]           \n",
    "\n",
    "# log-transform skewed numerics\n",
    "for c in [\"BTC\",\"USD\",\"Netflow_Bytes\"]:\n",
    "    if c in df_b.columns:\n",
    "        df_b[c] = np.log1p(df_b[c].clip(lower=0))\n",
    "\n",
    "X_b = df_b[num_b + low_card_cat + hash_cat]\n",
    "y_b = df_b[\"is_wannacry\"]\n",
    "\n",
    "Xtr_b, Xte_b, ytr_b, yte_b = train_test_split(\n",
    "    X_b, y_b, test_size=0.2, stratify=y_b, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "neg = (ytr_b == 0).sum()\n",
    "pos = (ytr_b == 1).sum()\n",
    "spw = neg / max(1, pos)\n",
    "\n",
    "\n",
    "pre_b = ColumnTransformer(\n",
    "    [\n",
    "        (\"num\",  \"passthrough\", num_b),\n",
    "        (\"lowcat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), low_card_cat),\n",
    "        (\"hash\", FeatureHasher(n_features=2**12, input_type=\"string\"), hash_cat),\n",
    "    ],\n",
    "    sparse_threshold=1.0  \n",
    ")\n",
    "\n",
    "\n",
    "pre_b.fit(Xtr_b, ytr_b)\n",
    "Xtr_t = pre_b.transform(Xtr_b)\n",
    "Xte_t = pre_b.transform(Xte_b)\n",
    "\n",
    "# ---------- XGBoost ----------\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    eval_metric=\"aucpr\",          # focus on positive class\n",
    "    scale_pos_weight=spw,         # handle imbalance\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=30\n",
    ")\n",
    "\n",
    "xgb.fit(\n",
    "    Xtr_t, ytr_b,\n",
    "    eval_set=[(Xte_t, yte_b)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Wrap back into a pipeline for later .predict/.predict_proba and saving\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe_b = Pipeline([(\"prep\", pre_b), (\"clf\", xgb)])\n",
    "\n",
    "# ---------- Threshold tuning ----------\n",
    "proba = pipe_b.predict_proba(Xte_b)[:,1]\n",
    "prec, rec, thr = precision_recall_curve(yte_b, proba)\n",
    "\n",
    "# choose threshold that maximizes F1 on the validation set\n",
    "f1 = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "best_idx = np.nanargmax(f1)\n",
    "best_thr = thr[best_idx] if best_idx < len(thr) else 0.5\n",
    "\n",
    "print(f\"Chosen threshold (max F1): {best_thr:.3f}  |  PR-AUC-guided\")\n",
    "\n",
    "pred_b = (proba >= best_thr).astype(int)\n",
    "print(\"=== BEHAV XGB (WannaCry vs Not) with tuned threshold ===\")\n",
    "print(confusion_matrix(yte_b, pred_b))\n",
    "print(classification_report(yte_b, pred_b, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "558e7ee9-08ec-47b1-bcf7-7018470f81d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richa\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen threshold (max F1): 0.690\n",
      "=== BEHAV XGB (WannaCry vs Not) with tuned threshold ===\n",
      "[[22643  3944]\n",
      " [ 1506  1716]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.938     0.852     0.893     26587\n",
      "           1      0.303     0.533     0.386      3222\n",
      "\n",
      "    accuracy                          0.817     29809\n",
      "   macro avg      0.620     0.692     0.639     29809\n",
      "weighted avg      0.869     0.817     0.838     29809\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richa\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# ---------- Build label ----------\n",
    "df_b = df_behav.copy()\n",
    "df_b[\"is_wannacry\"] = (df_b[\"Family\"] == \"WannaCry\").astype(int)\n",
    "\n",
    "# columns\n",
    "num_b        = [\"Time\",\"Netflow_Bytes\",\"BTC\",\"USD\",\"Port\",\"Clusters\"]\n",
    "low_card_cat = [\"Protocol\",\"Flag\",\"Threats\"]\n",
    "hash_cat     = [\"SeedAddress\",\"ExpAddress\"]          # <- make sure both exist\n",
    "\n",
    "# fill NA and log-transform skewed numerics\n",
    "for c in [\"BTC\",\"USD\",\"Netflow_Bytes\"]:\n",
    "    if c in df_b.columns:\n",
    "        df_b[c] = np.log1p(df_b[c].clip(lower=0))\n",
    "\n",
    "# hasher branch needs lists-of-strings per row\n",
    "to_list = FunctionTransformer(lambda X: X.astype(str).values.tolist(), validate=False)\n",
    "\n",
    "hash_branch = Pipeline([\n",
    "    (\"to_list\", to_list),\n",
    "    (\"hasher\", FeatureHasher(n_features=2**12, input_type=\"string\"))\n",
    "])\n",
    "\n",
    "pre_b = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", num_b),\n",
    "        (\"lowcat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), low_card_cat),\n",
    "        (\"hash\", hash_branch, hash_cat),\n",
    "    ],\n",
    "    sparse_threshold=1.0   # keep sparse; XGBoost handles it\n",
    ")\n",
    "\n",
    "X_b = df_b[num_b + low_card_cat + hash_cat]\n",
    "y_b = df_b[\"is_wannacry\"]\n",
    "\n",
    "Xtr_b, Xte_b, ytr_b, yte_b = train_test_split(\n",
    "    X_b, y_b, test_size=0.20, stratify=y_b, random_state=42\n",
    ")\n",
    "\n",
    "# class imbalance handling\n",
    "neg = (ytr_b == 0).sum()\n",
    "pos = (ytr_b == 1).sum()\n",
    "spw = neg / max(1, pos)\n",
    "\n",
    "# model\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    eval_metric=\"aucpr\",\n",
    "    scale_pos_weight=spw,\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=30\n",
    ")\n",
    "\n",
    "# fit with early stopping (use transformed X)\n",
    "Xtr_t = pre_b.fit_transform(Xtr_b, ytr_b)\n",
    "Xte_t = pre_b.transform(Xte_b)\n",
    "\n",
    "xgb.fit(Xtr_t, ytr_b, eval_set=[(Xte_t, yte_b)], verbose=False)\n",
    "\n",
    "# wrap as a single pipeline for later .predict/.predict_proba\n",
    "pipe_b = Pipeline([(\"prep\", pre_b), (\"clf\", xgb)])\n",
    "\n",
    "# ---------- threshold tuning on validation ----------\n",
    "proba = pipe_b.predict_proba(Xte_b)[:, 1]\n",
    "prec, rec, thr = precision_recall_curve(yte_b, proba)\n",
    "f1 = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "best_idx = np.nanargmax(f1)\n",
    "best_thr = thr[best_idx] if best_idx < len(thr) else 0.5\n",
    "print(f\"Chosen threshold (max F1): {best_thr:.3f}\")\n",
    "\n",
    "pred_b = (proba >= best_thr).astype(int)\n",
    "print(\"=== BEHAV XGB (WannaCry vs Not) with tuned threshold ===\")\n",
    "print(confusion_matrix(yte_b, pred_b))\n",
    "print(classification_report(yte_b, pred_b, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54a4afa1-412b-41f4-9e2e-27944f4c3d0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:Protocol: object, Flag: object, Threats: object, ProtoFlag: object, SeedAddress: object, ExpAddress: object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\data.py:407\u001b[39m, in \u001b[36mpandas_feature_info\u001b[39m\u001b[34m(data, meta, feature_names, feature_types, enable_categorical)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     new_feature_types.append(\u001b[43m_pandas_dtype_mapper\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[31mKeyError\u001b[39m: 'object'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 90\u001b[39m\n\u001b[32m     72\u001b[39m pipe_b = Pipeline([\n\u001b[32m     73\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mprep\u001b[39m\u001b[33m\"\u001b[39m, pre_b),\n\u001b[32m     74\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mclf\u001b[39m\u001b[33m\"\u001b[39m, XGBClassifier(\n\u001b[32m   (...)\u001b[39m\u001b[32m     86\u001b[39m     ))\n\u001b[32m     87\u001b[39m ])\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# proper pipeline fit (avoids the FutureWarning)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[43mpipe_b\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mXtr_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytr_b\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclf__eval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXte_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myte_b\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclf__verbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     94\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# ---------- threshold tuning ----------\u001b[39;00m\n\u001b[32m     97\u001b[39m proba = pipe_b.predict_proba(Xte_b)[:, \u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:663\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    657\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    658\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    659\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    660\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    661\u001b[39m             all_params=params,\n\u001b[32m    662\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\sklearn.py:1664\u001b[39m, in \u001b[36mXGBClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1659\u001b[39m     params[\u001b[33m\"\u001b[39m\u001b[33mnum_class\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.n_classes_\n\u001b[32m   1661\u001b[39m model, metric, params, feature_weights = \u001b[38;5;28mself\u001b[39m._configure_fit(\n\u001b[32m   1662\u001b[39m     xgb_model, params, feature_weights\n\u001b[32m   1663\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1664\u001b[39m train_dmatrix, evals = \u001b[43m_wrap_evaluation_matrices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1665\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1666\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1667\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1670\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1673\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1674\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1676\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_group\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1677\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_qid\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_dmatrix\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1679\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1680\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1681\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1683\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = train(\n\u001b[32m   1684\u001b[39m     params,\n\u001b[32m   1685\u001b[39m     train_dmatrix,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1694\u001b[39m     callbacks=\u001b[38;5;28mself\u001b[39m.callbacks,\n\u001b[32m   1695\u001b[39m )\n\u001b[32m   1697\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.objective):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\sklearn.py:679\u001b[39m, in \u001b[36m_wrap_evaluation_matrices\u001b[39m\u001b[34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[39m\n\u001b[32m    677\u001b[39m         evals.append(train_dmatrix)\n\u001b[32m    678\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m679\u001b[39m         m = \u001b[43mcreate_dmatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m            \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_group\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m            \u001b[49m\u001b[43mqid\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_qid\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m            \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m            \u001b[49m\u001b[43mref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    691\u001b[39m         evals.append(m)\n\u001b[32m    692\u001b[39m nevals = \u001b[38;5;28mlen\u001b[39m(evals)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\sklearn.py:1137\u001b[39m, in \u001b[36mXGBModel._create_dmatrix\u001b[39m\u001b[34m(self, ref, **kwargs)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _can_use_qdm(\u001b[38;5;28mself\u001b[39m.tree_method, \u001b[38;5;28mself\u001b[39m.device) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.booster != \u001b[33m\"\u001b[39m\u001b[33mgblinear\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1136\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQuantileDMatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnthread\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bin\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_bin\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1140\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[39;00m\n\u001b[32m   1141\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\core.py:1614\u001b[39m, in \u001b[36mQuantileDMatrix.__init__\u001b[39m\u001b[34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, max_quantile_batches, data_split_mode)\u001b[39m\n\u001b[32m   1594\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m   1595\u001b[39m         info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1596\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m   (...)\u001b[39m\u001b[32m   1607\u001b[39m         )\n\u001b[32m   1608\u001b[39m     ):\n\u001b[32m   1609\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1610\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mIf data iterator is used as input, data like label should be \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1611\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mspecified as batch argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1612\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1614\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1615\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1616\u001b[39m \u001b[43m    \u001b[49m\u001b[43mref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1617\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1618\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1619\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1620\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1621\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1627\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_quantile_blocks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_quantile_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1629\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\core.py:1678\u001b[39m, in \u001b[36mQuantileDMatrix._init\u001b[39m\u001b[34m(self, data, ref, enable_categorical, max_quantile_blocks, **meta)\u001b[39m\n\u001b[32m   1663\u001b[39m config = make_jcargs(\n\u001b[32m   1664\u001b[39m     nthread=\u001b[38;5;28mself\u001b[39m.nthread,\n\u001b[32m   1665\u001b[39m     missing=\u001b[38;5;28mself\u001b[39m.missing,\n\u001b[32m   1666\u001b[39m     max_bin=\u001b[38;5;28mself\u001b[39m.max_bin,\n\u001b[32m   1667\u001b[39m     max_quantile_blocks=max_quantile_blocks,\n\u001b[32m   1668\u001b[39m )\n\u001b[32m   1669\u001b[39m ret = _LIB.XGQuantileDMatrixCreateFromCallback(\n\u001b[32m   1670\u001b[39m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1671\u001b[39m     it.proxy.handle,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1676\u001b[39m     ctypes.byref(handle),\n\u001b[32m   1677\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1678\u001b[39m \u001b[43mit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1679\u001b[39m \u001b[38;5;66;03m# delay check_call to throw intermediate exception first\u001b[39;00m\n\u001b[32m   1680\u001b[39m _check_call(ret)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\core.py:572\u001b[39m, in \u001b[36mDataIter.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    570\u001b[39m exc = \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    571\u001b[39m \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m572\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\core.py:553\u001b[39m, in \u001b[36mDataIter._handle_exception\u001b[39m\u001b[34m(self, fn, dft_ret)\u001b[39m\n\u001b[32m    550\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dft_ret\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    555\u001b[39m     \u001b[38;5;66;03m# Defer the exception in order to return 0 and stop the iteration.\u001b[39;00m\n\u001b[32m    556\u001b[39m     \u001b[38;5;66;03m# Exception inside a ctype callback function has no effect except\u001b[39;00m\n\u001b[32m    557\u001b[39m     \u001b[38;5;66;03m# for printing to stderr (doesn't stop the execution).\u001b[39;00m\n\u001b[32m    558\u001b[39m     tb = sys.exc_info()[\u001b[32m2\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\core.py:640\u001b[39m, in \u001b[36mDataIter._next_wrapper.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    638\u001b[39m     \u001b[38;5;28mself\u001b[39m._temporary_data = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    639\u001b[39m \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handle_exception(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m), \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\data.py:1654\u001b[39m, in \u001b[36mSingleBatchInternalIter.next\u001b[39m\u001b[34m(self, input_data)\u001b[39m\n\u001b[32m   1652\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1653\u001b[39m \u001b[38;5;28mself\u001b[39m.it += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1654\u001b[39m \u001b[43minput_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1655\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\core.py:620\u001b[39m, in \u001b[36mDataIter._next_wrapper.<locals>.input_data\u001b[39m\u001b[34m(data, feature_names, feature_types, **kwargs)\u001b[39m\n\u001b[32m    618\u001b[39m     new, cat_codes, feature_names, feature_types = \u001b[38;5;28mself\u001b[39m._temporary_data\n\u001b[32m    619\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m     new, cat_codes, feature_names, feature_types = \u001b[43m_proxy_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_enable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[38;5;66;03m# Stage the data, meta info are copied inside C++ MetaInfo.\u001b[39;00m\n\u001b[32m    627\u001b[39m \u001b[38;5;28mself\u001b[39m._temporary_data = (new, cat_codes, feature_names, feature_types)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\data.py:1707\u001b[39m, in \u001b[36m_proxy_transform\u001b[39m\u001b[34m(data, feature_names, feature_types, enable_categorical)\u001b[39m\n\u001b[32m   1705\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df_pa, \u001b[38;5;28;01mNone\u001b[39;00m, feature_names, feature_types\n\u001b[32m   1706\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_pandas_df(data):\n\u001b[32m-> \u001b[39m\u001b[32m1707\u001b[39m     df, feature_names, feature_types = \u001b[43m_transform_pandas_df\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1708\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_types\u001b[49m\n\u001b[32m   1709\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1710\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df, \u001b[38;5;28;01mNone\u001b[39;00m, feature_names, feature_types\n\u001b[32m   1711\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mValue type is not supported for data iterator:\u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(data)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\data.py:640\u001b[39m, in \u001b[36m_transform_pandas_df\u001b[39m\u001b[34m(data, enable_categorical, feature_names, feature_types, meta)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data.columns) > \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m meta \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _matrix_meta:\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataFrame for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeta\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cannot have multiple columns\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m feature_names, feature_types = \u001b[43mpandas_feature_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_categorical\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    644\u001b[39m arrays = pandas_transform_data(data)\n\u001b[32m    645\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m PandasTransformed(arrays), feature_names, feature_types\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\data.py:409\u001b[39m, in \u001b[36mpandas_feature_info\u001b[39m\u001b[34m(data, meta, feature_names, feature_types, enable_categorical)\u001b[39m\n\u001b[32m    407\u001b[39m             new_feature_types.append(_pandas_dtype_mapper[dtype.name])\n\u001b[32m    408\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m             \u001b[43m_invalid_dataframe_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m feature_types \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m meta \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    412\u001b[39m     feature_types = new_feature_types\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\xgboost\\data.py:372\u001b[39m, in \u001b[36m_invalid_dataframe_dtype\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    370\u001b[39m type_err = \u001b[33m\"\u001b[39m\u001b[33mDataFrame.dtypes for data must be int, float, bool or category.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    371\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtype_err\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_ENABLE_CAT_ERR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[31mValueError\u001b[39m: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:Protocol: object, Flag: object, Threats: object, ProtoFlag: object, SeedAddress: object, ExpAddress: object"
     ]
    }
   ],
   "source": [
    "# === Behavioral model (UGRansome) with quick wins ===\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# ---------- Label ----------\n",
    "df_b = df_behav.copy()\n",
    "df_b[\"is_wannacry\"] = (df_b[\"Family\"] == \"WannaCry\").astype(int)\n",
    "\n",
    "# ---------- Feature engineering ----------\n",
    "# 1) log-transform skewed numerics\n",
    "for c in [\"BTC\",\"USD\",\"Netflow_Bytes\"]:\n",
    "    if c in df_b.columns:\n",
    "        df_b[c] = np.log1p(df_b[c].clip(lower=0))\n",
    "\n",
    "# 2) interaction: Protocol+Flag  (nonleaky)\n",
    "df_b[\"ProtoFlag\"] = df_b[\"Protocol\"].astype(str) + \"_\" + df_b[\"Flag\"].astype(str)\n",
    "\n",
    "# 3) bucket Port to reduce noise\n",
    "def bucket_port(p):\n",
    "    if p is None or (hasattr(p, \"isna\") and p.isna()):\n",
    "        return -1\n",
    "    p = int(p)\n",
    "    if p in [80, 443, 53, 25, 110, 143, 995, 993]:\n",
    "        return p\n",
    "    if p < 1024:   return 1000\n",
    "    if p < 10000:  return 10000\n",
    "    return 65535\n",
    "df_b[\"PortBucket\"] = df_b[\"Port\"].apply(bucket_port)\n",
    "\n",
    "# 4) presence flags for econ features\n",
    "df_b[\"has_BTC\"] = (df_b[\"BTC\"] > 0).astype(int)\n",
    "df_b[\"has_USD\"] = (df_b[\"USD\"] > 0).astype(int)\n",
    "\n",
    "# ---------- Column sets ----------\n",
    "num_b        = [\"Time\",\"Netflow_Bytes\",\"BTC\",\"USD\",\"Clusters\"]     # Port moved to bucketed categorical\n",
    "low_card_cat = [\"Protocol\",\"Flag\",\"Threats\",\"ProtoFlag\",\"PortBucket\",\"has_BTC\",\"has_USD\"]\n",
    "hash_cat     = [\"SeedAddress\",\"ExpAddress\"]   # keep hashed\n",
    "\n",
    "# hasher branch needs list-of-strings per row\n",
    "to_list = FunctionTransformer(lambda X: X.astype(str).values.tolist(), validate=False)\n",
    "hash_branch = Pipeline([\n",
    "    (\"to_list\", to_list),\n",
    "    (\"hasher\", FeatureHasher(n_features=2**13, input_type=\"string\"))  # a bit wider than before\n",
    "])\n",
    "\n",
    "pre_b = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\",    \"passthrough\", num_b),\n",
    "        (\"lowcat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), low_card_cat),\n",
    "        (\"hash\",   hash_branch, hash_cat),\n",
    "    ],\n",
    "    sparse_threshold=1.0\n",
    ")\n",
    "\n",
    "X_b = df_b[num_b + low_card_cat + hash_cat]\n",
    "y_b = df_b[\"is_wannacry\"]\n",
    "\n",
    "Xtr_b, Xte_b, ytr_b, yte_b = train_test_split(\n",
    "    X_b, y_b, test_size=0.20, stratify=y_b, random_state=42\n",
    ")\n",
    "\n",
    "# class imbalance\n",
    "neg, pos = int((ytr_b==0).sum()), int((ytr_b==1).sum())\n",
    "spw = neg / max(1, pos)              # you can bump this by +1020% to push recall\n",
    "\n",
    "pipe_b = Pipeline([\n",
    "    (\"prep\", pre_b),\n",
    "    (\"clf\", XGBClassifier(\n",
    "        n_estimators=700,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.045,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        eval_metric=\"aucpr\",\n",
    "        scale_pos_weight=spw,        # try spw*1.2 if you want more recall\n",
    "        tree_method=\"hist\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        early_stopping_rounds=30\n",
    "    ))\n",
    "])\n",
    "\n",
    "# proper pipeline fit (avoids the FutureWarning)\n",
    "pipe_b.fit(\n",
    "    Xtr_b, ytr_b,\n",
    "    clf__eval_set=[(Xte_b, yte_b)],\n",
    "    clf__verbose=False\n",
    ")\n",
    "\n",
    "# ---------- threshold tuning ----------\n",
    "proba = pipe_b.predict_proba(Xte_b)[:, 1]\n",
    "prec, rec, thr = precision_recall_curve(yte_b, proba)\n",
    "\n",
    "# Option A: choose threshold that maximizes F1\n",
    "f1 = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "best_idx = np.nanargmax(f1)\n",
    "best_thr = thr[best_idx] if best_idx < len(thr) else 0.5\n",
    "\n",
    "# Option B (if you want a target recall per NFR, e.g., 0.85):\n",
    "# target_recall = 0.85\n",
    "# idx = np.argmax(rec >= target_recall)\n",
    "# best_thr = thr[idx] if idx < len(thr) else best_thr\n",
    "\n",
    "print(f\"Chosen threshold: {best_thr:.3f}\")\n",
    "pred_b = (proba >= best_thr).astype(int)\n",
    "\n",
    "print(\"=== BEHAV XGB (WannaCry vs Not)  tuned threshold ===\")\n",
    "print(confusion_matrix(yte_b, pred_b))\n",
    "print(classification_report(yte_b, pred_b, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d23ed34-6d46-4303-a4e5-3a2c2953192a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richa\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\richa\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen threshold (max F1): 0.639\n",
      "=== BEHAV XGB (WannaCry vs Not) with tuned threshold ===\n",
      "[[20974  5613]\n",
      " [ 1123  2099]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.949     0.789     0.862     26587\n",
      "           1      0.272     0.651     0.384      3222\n",
      "\n",
      "    accuracy                          0.774     29809\n",
      "   macro avg      0.611     0.720     0.623     29809\n",
      "weighted avg      0.876     0.774     0.810     29809\n",
      "\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function <lambda> at 0x00000259DF771800>: it's not found as __main__.<lambda>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPicklingError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 133\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# --------------------------\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# 6) Save artifacts for your app\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# --------------------------\u001b[39;00m\n\u001b[32m    132\u001b[39m MODEL_DIR = Path(\u001b[33m\"\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m\"\u001b[39m); MODEL_DIR.mkdir(exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpre_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbehav_preprocessor.joblib\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m joblib.dump(xgb,   MODEL_DIR / \u001b[33m\"\u001b[39m\u001b[33mbehav_xgb.joblib\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(MODEL_DIR / \u001b[33m\"\u001b[39m\u001b[33mbehav_threshold.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\joblib\\numpy_pickle.py:600\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(value, filename, compress, protocol)\u001b[39m\n\u001b[32m    598\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n\u001b[32m    599\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m         \u001b[43mNumpyPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    602\u001b[39m     NumpyPickler(filename, protocol=protocol).dump(value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:487\u001b[39m, in \u001b[36m_Pickler.dump\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.proto >= \u001b[32m4\u001b[39m:\n\u001b[32m    486\u001b[39m     \u001b[38;5;28mself\u001b[39m.framer.start_framing()\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[38;5;28mself\u001b[39m.write(STOP)\n\u001b[32m    489\u001b[39m \u001b[38;5;28mself\u001b[39m.framer.end_framing()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\joblib\\numpy_pickle.py:395\u001b[39m, in \u001b[36mNumpyPickler.save\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    392\u001b[39m     wrapper.write_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:603\u001b[39m, in \u001b[36m_Pickler.save\u001b[39m\u001b[34m(self, obj, save_persistent_id)\u001b[39m\n\u001b[32m    599\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PicklingError(\u001b[33m\"\u001b[39m\u001b[33mTuple returned by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m must have \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    600\u001b[39m                         \u001b[33m\"\u001b[39m\u001b[33mtwo to six elements\u001b[39m\u001b[33m\"\u001b[39m % reduce)\n\u001b[32m    602\u001b[39m \u001b[38;5;66;03m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m603\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:717\u001b[39m, in \u001b[36m_Pickler.save_reduce\u001b[39m\u001b[34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[39m\n\u001b[32m    715\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    716\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m state_setter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m717\u001b[39m         \u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    718\u001b[39m         write(BUILD)\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    720\u001b[39m         \u001b[38;5;66;03m# If a state_setter is specified, call it instead of load_build\u001b[39;00m\n\u001b[32m    721\u001b[39m         \u001b[38;5;66;03m# to update obj's with its previous state.\u001b[39;00m\n\u001b[32m    722\u001b[39m         \u001b[38;5;66;03m# First, push state_setter and its tuple of expected arguments\u001b[39;00m\n\u001b[32m    723\u001b[39m         \u001b[38;5;66;03m# (obj, state) onto the stack.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\joblib\\numpy_pickle.py:395\u001b[39m, in \u001b[36mNumpyPickler.save\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    392\u001b[39m     wrapper.write_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:560\u001b[39m, in \u001b[36m_Pickler.save\u001b[39m\u001b[34m(self, obj, save_persistent_id)\u001b[39m\n\u001b[32m    558\u001b[39m f = \u001b[38;5;28mself\u001b[39m.dispatch.get(t)\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[32m    561\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    563\u001b[39m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:972\u001b[39m, in \u001b[36m_Pickler.save_dict\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    969\u001b[39m     \u001b[38;5;28mself\u001b[39m.write(MARK + DICT)\n\u001b[32m    971\u001b[39m \u001b[38;5;28mself\u001b[39m.memoize(obj)\n\u001b[32m--> \u001b[39m\u001b[32m972\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch_setitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:998\u001b[39m, in \u001b[36m_Pickler._batch_setitems\u001b[39m\u001b[34m(self, items)\u001b[39m\n\u001b[32m    996\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tmp:\n\u001b[32m    997\u001b[39m         save(k)\n\u001b[32m--> \u001b[39m\u001b[32m998\u001b[39m         \u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    999\u001b[39m     write(SETITEMS)\n\u001b[32m   1000\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m n:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\joblib\\numpy_pickle.py:395\u001b[39m, in \u001b[36mNumpyPickler.save\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    392\u001b[39m     wrapper.write_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:560\u001b[39m, in \u001b[36m_Pickler.save\u001b[39m\u001b[34m(self, obj, save_persistent_id)\u001b[39m\n\u001b[32m    558\u001b[39m f = \u001b[38;5;28mself\u001b[39m.dispatch.get(t)\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[32m    561\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    563\u001b[39m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:932\u001b[39m, in \u001b[36m_Pickler.save_list\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    929\u001b[39m     \u001b[38;5;28mself\u001b[39m.write(MARK + LIST)\n\u001b[32m    931\u001b[39m \u001b[38;5;28mself\u001b[39m.memoize(obj)\n\u001b[32m--> \u001b[39m\u001b[32m932\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch_appends\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:956\u001b[39m, in \u001b[36m_Pickler._batch_appends\u001b[39m\u001b[34m(self, items)\u001b[39m\n\u001b[32m    954\u001b[39m     write(MARK)\n\u001b[32m    955\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tmp:\n\u001b[32m--> \u001b[39m\u001b[32m956\u001b[39m         \u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    957\u001b[39m     write(APPENDS)\n\u001b[32m    958\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m n:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\joblib\\numpy_pickle.py:395\u001b[39m, in \u001b[36mNumpyPickler.save\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    392\u001b[39m     wrapper.write_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:560\u001b[39m, in \u001b[36m_Pickler.save\u001b[39m\u001b[34m(self, obj, save_persistent_id)\u001b[39m\n\u001b[32m    558\u001b[39m f = \u001b[38;5;28mself\u001b[39m.dispatch.get(t)\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[32m    561\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    563\u001b[39m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:887\u001b[39m, in \u001b[36m_Pickler.save_tuple\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    885\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n <= \u001b[32m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.proto >= \u001b[32m2\u001b[39m:\n\u001b[32m    886\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m obj:\n\u001b[32m--> \u001b[39m\u001b[32m887\u001b[39m         \u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    888\u001b[39m     \u001b[38;5;66;03m# Subtle.  Same as in the big comment below.\u001b[39;00m\n\u001b[32m    889\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mid\u001b[39m(obj) \u001b[38;5;129;01min\u001b[39;00m memo:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\joblib\\numpy_pickle.py:395\u001b[39m, in \u001b[36mNumpyPickler.save\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    392\u001b[39m     wrapper.write_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:603\u001b[39m, in \u001b[36m_Pickler.save\u001b[39m\u001b[34m(self, obj, save_persistent_id)\u001b[39m\n\u001b[32m    599\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PicklingError(\u001b[33m\"\u001b[39m\u001b[33mTuple returned by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m must have \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    600\u001b[39m                         \u001b[33m\"\u001b[39m\u001b[33mtwo to six elements\u001b[39m\u001b[33m\"\u001b[39m % reduce)\n\u001b[32m    602\u001b[39m \u001b[38;5;66;03m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m603\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:717\u001b[39m, in \u001b[36m_Pickler.save_reduce\u001b[39m\u001b[34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[39m\n\u001b[32m    715\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    716\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m state_setter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m717\u001b[39m         \u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    718\u001b[39m         write(BUILD)\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    720\u001b[39m         \u001b[38;5;66;03m# If a state_setter is specified, call it instead of load_build\u001b[39;00m\n\u001b[32m    721\u001b[39m         \u001b[38;5;66;03m# to update obj's with its previous state.\u001b[39;00m\n\u001b[32m    722\u001b[39m         \u001b[38;5;66;03m# First, push state_setter and its tuple of expected arguments\u001b[39;00m\n\u001b[32m    723\u001b[39m         \u001b[38;5;66;03m# (obj, state) onto the stack.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\joblib\\numpy_pickle.py:395\u001b[39m, in \u001b[36mNumpyPickler.save\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    392\u001b[39m     wrapper.write_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:560\u001b[39m, in \u001b[36m_Pickler.save\u001b[39m\u001b[34m(self, obj, save_persistent_id)\u001b[39m\n\u001b[32m    558\u001b[39m f = \u001b[38;5;28mself\u001b[39m.dispatch.get(t)\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[32m    561\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    563\u001b[39m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:972\u001b[39m, in \u001b[36m_Pickler.save_dict\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    969\u001b[39m     \u001b[38;5;28mself\u001b[39m.write(MARK + DICT)\n\u001b[32m    971\u001b[39m \u001b[38;5;28mself\u001b[39m.memoize(obj)\n\u001b[32m--> \u001b[39m\u001b[32m972\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch_setitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:998\u001b[39m, in \u001b[36m_Pickler._batch_setitems\u001b[39m\u001b[34m(self, items)\u001b[39m\n\u001b[32m    996\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tmp:\n\u001b[32m    997\u001b[39m         save(k)\n\u001b[32m--> \u001b[39m\u001b[32m998\u001b[39m         \u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    999\u001b[39m     write(SETITEMS)\n\u001b[32m   1000\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m n:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\joblib\\numpy_pickle.py:395\u001b[39m, in \u001b[36mNumpyPickler.save\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    392\u001b[39m     wrapper.write_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:560\u001b[39m, in \u001b[36m_Pickler.save\u001b[39m\u001b[34m(self, obj, save_persistent_id)\u001b[39m\n\u001b[32m    558\u001b[39m f = \u001b[38;5;28mself\u001b[39m.dispatch.get(t)\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[32m    561\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    563\u001b[39m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:932\u001b[39m, in \u001b[36m_Pickler.save_list\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    929\u001b[39m     \u001b[38;5;28mself\u001b[39m.write(MARK + LIST)\n\u001b[32m    931\u001b[39m \u001b[38;5;28mself\u001b[39m.memoize(obj)\n\u001b[32m--> \u001b[39m\u001b[32m932\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch_appends\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:956\u001b[39m, in \u001b[36m_Pickler._batch_appends\u001b[39m\u001b[34m(self, items)\u001b[39m\n\u001b[32m    954\u001b[39m     write(MARK)\n\u001b[32m    955\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tmp:\n\u001b[32m--> \u001b[39m\u001b[32m956\u001b[39m         \u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    957\u001b[39m     write(APPENDS)\n\u001b[32m    958\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m n:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\joblib\\numpy_pickle.py:395\u001b[39m, in \u001b[36mNumpyPickler.save\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    392\u001b[39m     wrapper.write_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:560\u001b[39m, in \u001b[36m_Pickler.save\u001b[39m\u001b[34m(self, obj, save_persistent_id)\u001b[39m\n\u001b[32m    558\u001b[39m f = \u001b[38;5;28mself\u001b[39m.dispatch.get(t)\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[32m    561\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    563\u001b[39m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:887\u001b[39m, in \u001b[36m_Pickler.save_tuple\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    885\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n <= \u001b[32m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.proto >= \u001b[32m2\u001b[39m:\n\u001b[32m    886\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m obj:\n\u001b[32m--> \u001b[39m\u001b[32m887\u001b[39m         \u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    888\u001b[39m     \u001b[38;5;66;03m# Subtle.  Same as in the big comment below.\u001b[39;00m\n\u001b[32m    889\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mid\u001b[39m(obj) \u001b[38;5;129;01min\u001b[39;00m memo:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\joblib\\numpy_pickle.py:395\u001b[39m, in \u001b[36mNumpyPickler.save\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    392\u001b[39m     wrapper.write_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:603\u001b[39m, in \u001b[36m_Pickler.save\u001b[39m\u001b[34m(self, obj, save_persistent_id)\u001b[39m\n\u001b[32m    599\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PicklingError(\u001b[33m\"\u001b[39m\u001b[33mTuple returned by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m must have \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    600\u001b[39m                         \u001b[33m\"\u001b[39m\u001b[33mtwo to six elements\u001b[39m\u001b[33m\"\u001b[39m % reduce)\n\u001b[32m    602\u001b[39m \u001b[38;5;66;03m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m603\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:717\u001b[39m, in \u001b[36m_Pickler.save_reduce\u001b[39m\u001b[34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[39m\n\u001b[32m    715\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    716\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m state_setter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m717\u001b[39m         \u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    718\u001b[39m         write(BUILD)\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    720\u001b[39m         \u001b[38;5;66;03m# If a state_setter is specified, call it instead of load_build\u001b[39;00m\n\u001b[32m    721\u001b[39m         \u001b[38;5;66;03m# to update obj's with its previous state.\u001b[39;00m\n\u001b[32m    722\u001b[39m         \u001b[38;5;66;03m# First, push state_setter and its tuple of expected arguments\u001b[39;00m\n\u001b[32m    723\u001b[39m         \u001b[38;5;66;03m# (obj, state) onto the stack.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\joblib\\numpy_pickle.py:395\u001b[39m, in \u001b[36mNumpyPickler.save\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    392\u001b[39m     wrapper.write_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:560\u001b[39m, in \u001b[36m_Pickler.save\u001b[39m\u001b[34m(self, obj, save_persistent_id)\u001b[39m\n\u001b[32m    558\u001b[39m f = \u001b[38;5;28mself\u001b[39m.dispatch.get(t)\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[32m    561\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    563\u001b[39m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:972\u001b[39m, in \u001b[36m_Pickler.save_dict\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    969\u001b[39m     \u001b[38;5;28mself\u001b[39m.write(MARK + DICT)\n\u001b[32m    971\u001b[39m \u001b[38;5;28mself\u001b[39m.memoize(obj)\n\u001b[32m--> \u001b[39m\u001b[32m972\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch_setitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:998\u001b[39m, in \u001b[36m_Pickler._batch_setitems\u001b[39m\u001b[34m(self, items)\u001b[39m\n\u001b[32m    996\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tmp:\n\u001b[32m    997\u001b[39m         save(k)\n\u001b[32m--> \u001b[39m\u001b[32m998\u001b[39m         \u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    999\u001b[39m     write(SETITEMS)\n\u001b[32m   1000\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m n:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\joblib\\numpy_pickle.py:395\u001b[39m, in \u001b[36mNumpyPickler.save\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    392\u001b[39m     wrapper.write_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:560\u001b[39m, in \u001b[36m_Pickler.save\u001b[39m\u001b[34m(self, obj, save_persistent_id)\u001b[39m\n\u001b[32m    558\u001b[39m f = \u001b[38;5;28mself\u001b[39m.dispatch.get(t)\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[32m    561\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    563\u001b[39m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py:1071\u001b[39m, in \u001b[36m_Pickler.save_global\u001b[39m\u001b[34m(self, obj, name)\u001b[39m\n\u001b[32m   1069\u001b[39m     obj2, parent = _getattribute(module, name)\n\u001b[32m   1070\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1071\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PicklingError(\n\u001b[32m   1072\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt pickle \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m: it\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms not found as \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1073\u001b[39m         (obj, module_name, name)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1074\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1075\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m obj2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m obj:\n",
      "\u001b[31mPicklingError\u001b[39m: Can't pickle <function <lambda> at 0x00000259DF771800>: it's not found as __main__.<lambda>"
     ]
    }
   ],
   "source": [
    "# === BEHAVIORAL MODEL: WannaCry vs Not ===\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "# --------------------------\n",
    "# 1) Label + feature engineering\n",
    "# --------------------------\n",
    "df_b = df_behav.copy()\n",
    "\n",
    "# Target\n",
    "df_b[\"is_wannacry\"] = (df_b[\"Family\"] == \"WannaCry\").astype(int)\n",
    "\n",
    "# Interaction of low-card categoricals (often helps)\n",
    "df_b[\"ProtoFlag\"] = df_b[\"Protocol\"].astype(str) + \"_\" + df_b[\"Flag\"].astype(str)\n",
    "\n",
    "# Port buckets (reduce noise from rare ports)\n",
    "def bucket_port(p):\n",
    "    try:\n",
    "        p = int(p)\n",
    "    except Exception:\n",
    "        return -1\n",
    "    if p in [80, 443, 53, 25, 110, 143, 995, 993]:  # common ports kept as-is\n",
    "        return p\n",
    "    if p < 1024:   return 1000\n",
    "    if p < 10000:  return 10000\n",
    "    return 65535\n",
    "\n",
    "df_b[\"PortBucket\"] = df_b[\"Port\"].apply(bucket_port)\n",
    "\n",
    "# Binary presence flags for econ features\n",
    "df_b[\"has_BTC\"] = (df_b[\"BTC\"] > 0).astype(int)\n",
    "df_b[\"has_USD\"] = (df_b[\"USD\"] > 0).astype(int)\n",
    "\n",
    "# Log-transform skewed numerics (safe)\n",
    "for c in [\"BTC\", \"USD\", \"Netflow_Bytes\"]:\n",
    "    if c in df_b.columns:\n",
    "        df_b[c] = np.log1p(df_b[c].clip(lower=0))\n",
    "\n",
    "# --------------------------\n",
    "# 2) Column sets\n",
    "# --------------------------\n",
    "num_b        = [\"Time\", \"Netflow_Bytes\", \"BTC\", \"USD\", \"Clusters\"]\n",
    "low_card_cat = [\"Protocol\", \"Flag\", \"Threats\", \"ProtoFlag\", \"has_BTC\", \"has_USD\", \"PortBucket\"]\n",
    "hash_cat     = [\"SeedAddress\", \"ExpAddress\"]  # long text-like IDs -> hash\n",
    "\n",
    "# Guard for missing hashed columns\n",
    "hash_cat = [c for c in hash_cat if c in df_b.columns]\n",
    "\n",
    "# Raw feature matrix & label\n",
    "X_b = df_b[num_b + low_card_cat + hash_cat]\n",
    "y_b = df_b[\"is_wannacry\"]\n",
    "\n",
    "Xtr_b, Xte_b, ytr_b, yte_b = train_test_split(\n",
    "    X_b, y_b, test_size=0.20, stratify=y_b, random_state=42\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# 3) Preprocessor\n",
    "# --------------------------\n",
    "# Hasher branch: convert each row of selected columns into a list-of-strings\n",
    "to_list = FunctionTransformer(lambda X: X.astype(str).values.tolist(), validate=False)\n",
    "hash_branch = Pipeline([\n",
    "    (\"to_list\", to_list),\n",
    "    (\"hasher\", FeatureHasher(n_features=2**13, input_type=\"string\")),  # a bit richer than 2**12\n",
    "])\n",
    "\n",
    "pre_b = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\",    \"passthrough\",                                  num_b),\n",
    "        (\"lowcat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), low_card_cat),\n",
    "        (\"hash\",   hash_branch,                                     hash_cat),\n",
    "    ],\n",
    "    sparse_threshold=1.0,  # keep sparse; XGB is fine with CSR\n",
    ")\n",
    "\n",
    "# Fit preprocessor on train and transform both splits (IMPORTANT)\n",
    "pre_b.fit(Xtr_b, ytr_b)\n",
    "Xtr_t = pre_b.transform(Xtr_b)\n",
    "Xte_t = pre_b.transform(Xte_b)\n",
    "\n",
    "# --------------------------\n",
    "# 4) Model (XGBoost) + early stopping on transformed eval set\n",
    "# --------------------------\n",
    "neg = int((ytr_b == 0).sum())\n",
    "pos = int((ytr_b == 1).sum())\n",
    "spw = neg / max(1, pos)  # class imbalance handling\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=700,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.045,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    eval_metric=\"aucpr\",\n",
    "    scale_pos_weight=spw,\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=30,\n",
    ")\n",
    "\n",
    "xgb.fit(Xtr_t, ytr_b, eval_set=[(Xte_t, yte_b)], verbose=False)\n",
    "\n",
    "# --------------------------\n",
    "# 5) Threshold tuning on PR-curve (maximize F1 by default)\n",
    "# --------------------------\n",
    "proba = xgb.predict_proba(Xte_t)[:, 1]\n",
    "prec, rec, thr = precision_recall_curve(yte_b, proba)\n",
    "f1 = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "best_idx = np.nanargmax(f1)\n",
    "best_thr = float(thr[best_idx]) if best_idx < len(thr) else 0.5\n",
    "print(f\"Chosen threshold (max F1): {best_thr:.3f}\")\n",
    "\n",
    "pred_b = (proba >= best_thr).astype(int)\n",
    "print(\"=== BEHAV XGB (WannaCry vs Not) with tuned threshold ===\")\n",
    "print(confusion_matrix(yte_b, pred_b))\n",
    "print(classification_report(yte_b, pred_b, digits=3))\n",
    "\n",
    "# --------------------------\n",
    "# 6) Save artifacts for your app\n",
    "# --------------------------\n",
    "MODEL_DIR = Path(\"models\"); MODEL_DIR.mkdir(exist_ok=True)\n",
    "joblib.dump(pre_b, MODEL_DIR / \"behav_preprocessor.joblib\")\n",
    "joblib.dump(xgb,   MODEL_DIR / \"behav_xgb.joblib\")\n",
    "with open(MODEL_DIR / \"behav_threshold.json\", \"w\") as f:\n",
    "    json.dump({\"threshold\": best_thr}, f)\n",
    "print(\"Saved:\", list(MODEL_DIR.iterdir()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bbeea1ae-d542-496c-a6fa-33b79a0a456f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richa\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen threshold (max F1): 0.639\n",
      "=== BEHAV XGB (WannaCry vs Not) with tuned threshold ===\n",
      "[[20974  5613]\n",
      " [ 1123  2099]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.949     0.789     0.862     26587\n",
      "           1      0.272     0.651     0.384      3222\n",
      "\n",
      "    accuracy                          0.774     29809\n",
      "   macro avg      0.611     0.720     0.623     29809\n",
      "weighted avg      0.876     0.774     0.810     29809\n",
      "\n",
      " Saved: [WindowsPath('models/behav_preprocessor.joblib'), WindowsPath('models/behav_threshold.json'), WindowsPath('models/behav_wannacry_bundle.joblib'), WindowsPath('models/behav_xgb_pipeline.joblib'), WindowsPath('models/static_feature_names.joblib'), WindowsPath('models/static_rf.joblib'), WindowsPath('models/static_scaler.joblib')]\n",
      " Bundle verification passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richa\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# BEHAVIORAL (UGRansome)  FINAL TRAIN+SAVE BUNDLE\n",
    "# =========================\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "import joblib, json\n",
    "\n",
    "# -------- 0) Helper functions (pickle-safe, no lambdas) --------\n",
    "def to_list_func(X: pd.DataFrame):\n",
    "    \"\"\"FeatureHasher expects 'iterables of strings' per row.\"\"\"\n",
    "    return X.astype(str).values.tolist()\n",
    "\n",
    "def bucket_port(p):\n",
    "    if pd.isna(p): return -1\n",
    "    p = int(p)\n",
    "    if p in {80, 443, 53, 25, 110, 143, 995, 993}: return p\n",
    "    if p < 1024: return 1000\n",
    "    if p < 10000: return 10000\n",
    "    return 65535\n",
    "\n",
    "# -------- 1) Label + feature engineering --------\n",
    "df_b = df_behav.copy()\n",
    "df_b[\"is_wannacry\"] = (df_b[\"Family\"] == \"WannaCry\").astype(int)\n",
    "\n",
    "# Rich categorical signals\n",
    "df_b[\"ProtoFlag\"]  = df_b[\"Protocol\"].astype(str) + \"_\" + df_b[\"Flag\"].astype(str)\n",
    "df_b[\"PortBucket\"] = df_b[\"Port\"].apply(bucket_port)\n",
    "df_b[\"has_BTC\"]    = (df_b[\"BTC\"] > 0).astype(int)\n",
    "df_b[\"has_USD\"]    = (df_b[\"USD\"] > 0).astype(int)\n",
    "\n",
    "# Log-transform skewed numerics\n",
    "for c in [\"BTC\",\"USD\",\"Netflow_Bytes\"]:\n",
    "    if c in df_b.columns:\n",
    "        df_b[c] = np.log1p(df_b[c].clip(lower=0))\n",
    "\n",
    "# Column sets (schema)\n",
    "num_b        = [\"Time\",\"Netflow_Bytes\",\"BTC\",\"USD\",\"Clusters\"]   # Port handled via PortBucket\n",
    "low_card_cat = [\"Protocol\",\"Flag\",\"Threats\",\"ProtoFlag\",\"PortBucket\",\"has_BTC\",\"has_USD\"]\n",
    "hash_cat     = [\"SeedAddress\",\"ExpAddress\"]  # high-card strings -> hashing\n",
    "\n",
    "feature_schema = {\n",
    "    \"numeric\": num_b,\n",
    "    \"low_card_cat\": low_card_cat,\n",
    "    \"hash_cat\": hash_cat,\n",
    "}\n",
    "\n",
    "X_b = df_b[num_b + low_card_cat + hash_cat]\n",
    "y_b = df_b[\"is_wannacry\"]\n",
    "\n",
    "# Train/val split (stratified)\n",
    "Xtr_b, Xte_b, ytr_b, yte_b = train_test_split(\n",
    "    X_b, y_b, test_size=0.20, stratify=y_b, random_state=42\n",
    ")\n",
    "\n",
    "# Class imbalance (neg/pos) for scale_pos_weight\n",
    "neg = int((ytr_b == 0).sum())\n",
    "pos = int((ytr_b == 1).sum())\n",
    "spw = neg / max(1, pos)\n",
    "\n",
    "# -------- 2) Preprocessor (pickle-safe) --------\n",
    "hash_branch = Pipeline([\n",
    "    (\"to_list\", FunctionTransformer(to_list_func, validate=False)),\n",
    "    (\"hasher\",  FeatureHasher(n_features=2**13, input_type=\"string\")),\n",
    "])\n",
    "\n",
    "pre_b = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\",    \"passthrough\",                               num_b),\n",
    "        (\"lowcat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), low_card_cat),\n",
    "        (\"hash\",   hash_branch,                                 hash_cat),\n",
    "    ],\n",
    "    sparse_threshold=1.0  # keep sparse; XGB handles it\n",
    ")\n",
    "\n",
    "# Fit preprocessor and transform\n",
    "Xtr_t = pre_b.fit_transform(Xtr_b, ytr_b)\n",
    "Xte_t = pre_b.transform(Xte_b)\n",
    "\n",
    "# -------- 3) Train XGBoost on transformed matrices --------\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=700,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.045,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    eval_metric=\"aucpr\",\n",
    "    scale_pos_weight=spw,\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=30,\n",
    ")\n",
    "xgb.fit(Xtr_t, ytr_b, eval_set=[(Xte_t, yte_b)], verbose=False)\n",
    "\n",
    "# -------- 4) Threshold tuning (maximize F1) --------\n",
    "proba = xgb.predict_proba(Xte_t)[:, 1]\n",
    "prec, rec, thr = precision_recall_curve(yte_b, proba)\n",
    "f1 = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "best_idx = int(np.nanargmax(f1))\n",
    "best_thr = float(thr[best_idx]) if best_idx < len(thr) else 0.5\n",
    "print(f\"Chosen threshold (max F1): {best_thr:.3f}\")\n",
    "\n",
    "pred_b = (proba >= best_thr).astype(int)\n",
    "print(\"=== BEHAV XGB (WannaCry vs Not) with tuned threshold ===\")\n",
    "print(confusion_matrix(yte_b, pred_b))\n",
    "print(classification_report(yte_b, pred_b, digits=3))\n",
    "\n",
    "# -------- 5) Bundle & Save (no unfitted-pipeline warning, fully pickleable) --------\n",
    "MODEL_DIR = Path(\"models\"); MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "bundle = {\n",
    "    \"preprocessor\": pre_b,        # fitted ColumnTransformer\n",
    "    \"model\": xgb,                 # trained XGBClassifier\n",
    "    \"threshold\": best_thr,        # tuned decision threshold\n",
    "    \"schema\": feature_schema,     # for future validation\n",
    "}\n",
    "\n",
    "joblib.dump(bundle, MODEL_DIR / \"behav_wannacry_bundle.joblib\")\n",
    "\n",
    "with open(MODEL_DIR / \"behav_threshold.json\", \"w\") as f:\n",
    "    json.dump({\"threshold\": best_thr}, f)\n",
    "\n",
    "print(\" Saved:\", list(MODEL_DIR.iterdir()))\n",
    "\n",
    "# -------- 6) (Optional) Quick self-check of the saved bundle --------\n",
    "loaded = joblib.load(MODEL_DIR / \"behav_wannacry_bundle.joblib\")\n",
    "proba_check = loaded[\"model\"].predict_proba(loaded[\"preprocessor\"].transform(Xte_b))[:, 1]\n",
    "assert np.allclose(proba_check, proba, atol=1e-6), \"Predictions differ after reload!\"\n",
    "print(\" Bundle verification passed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "888c819e-e3ca-47d4-b846-e75241ac9ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboost\n",
      "  Downloading catboost-1.2.8-cp311-cp311-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting graphviz (from catboost)\n",
      "  Downloading graphviz-0.21-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\richa\\onedrive\\documents\\fyp2\\.venv\\lib\\site-packages (from catboost) (3.10.5)\n",
      "Requirement already satisfied: numpy<3.0,>=1.16.0 in c:\\users\\richa\\onedrive\\documents\\fyp2\\.venv\\lib\\site-packages (from catboost) (2.3.2)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\users\\richa\\onedrive\\documents\\fyp2\\.venv\\lib\\site-packages (from catboost) (2.3.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\richa\\onedrive\\documents\\fyp2\\.venv\\lib\\site-packages (from catboost) (1.16.1)\n",
      "Collecting plotly (from catboost)\n",
      "  Downloading plotly-6.3.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: six in c:\\users\\richa\\onedrive\\documents\\fyp2\\.venv\\lib\\site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\richa\\onedrive\\documents\\fyp2\\.venv\\lib\\site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\richa\\onedrive\\documents\\fyp2\\.venv\\lib\\site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\richa\\onedrive\\documents\\fyp2\\.venv\\lib\\site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\richa\\onedrive\\documents\\fyp2\\.venv\\lib\\site-packages (from matplotlib->catboost) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\richa\\onedrive\\documents\\fyp2\\.venv\\lib\\site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\richa\\onedrive\\documents\\fyp2\\.venv\\lib\\site-packages (from matplotlib->catboost) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\richa\\onedrive\\documents\\fyp2\\.venv\\lib\\site-packages (from matplotlib->catboost) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\richa\\onedrive\\documents\\fyp2\\.venv\\lib\\site-packages (from matplotlib->catboost) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\richa\\onedrive\\documents\\fyp2\\.venv\\lib\\site-packages (from matplotlib->catboost) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\richa\\onedrive\\documents\\fyp2\\.venv\\lib\\site-packages (from matplotlib->catboost) (3.2.3)\n",
      "Collecting narwhals>=1.15.1 (from plotly->catboost)\n",
      "  Downloading narwhals-2.1.1-py3-none-any.whl.metadata (11 kB)\n",
      "Downloading catboost-1.2.8-cp311-cp311-win_amd64.whl (102.5 MB)\n",
      "   ---------------------------------------- 0.0/102.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.8/102.5 MB 4.8 MB/s eta 0:00:22\n",
      "    --------------------------------------- 1.6/102.5 MB 4.4 MB/s eta 0:00:23\n",
      "   - -------------------------------------- 2.9/102.5 MB 4.8 MB/s eta 0:00:21\n",
      "   - -------------------------------------- 3.9/102.5 MB 5.1 MB/s eta 0:00:20\n",
      "   -- ------------------------------------- 5.2/102.5 MB 5.2 MB/s eta 0:00:19\n",
      "   -- ------------------------------------- 6.3/102.5 MB 5.1 MB/s eta 0:00:19\n",
      "   -- ------------------------------------- 7.1/102.5 MB 4.9 MB/s eta 0:00:20\n",
      "   -- ------------------------------------- 7.3/102.5 MB 4.9 MB/s eta 0:00:20\n",
      "   -- ------------------------------------- 7.3/102.5 MB 4.9 MB/s eta 0:00:20\n",
      "   --- ------------------------------------ 7.9/102.5 MB 3.7 MB/s eta 0:00:26\n",
      "   --- ------------------------------------ 8.9/102.5 MB 3.9 MB/s eta 0:00:25\n",
      "   --- ------------------------------------ 10.0/102.5 MB 4.0 MB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 10.7/102.5 MB 4.0 MB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 12.1/102.5 MB 4.1 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 13.1/102.5 MB 4.2 MB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 14.7/102.5 MB 4.4 MB/s eta 0:00:21\n",
      "   ------ --------------------------------- 15.7/102.5 MB 4.4 MB/s eta 0:00:20\n",
      "   ------ --------------------------------- 17.0/102.5 MB 4.5 MB/s eta 0:00:20\n",
      "   ------- -------------------------------- 18.1/102.5 MB 4.5 MB/s eta 0:00:19\n",
      "   ------- -------------------------------- 19.1/102.5 MB 4.6 MB/s eta 0:00:19\n",
      "   ------- -------------------------------- 20.4/102.5 MB 4.6 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 21.8/102.5 MB 4.7 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 22.8/102.5 MB 4.7 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 23.6/102.5 MB 4.7 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 24.4/102.5 MB 4.7 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 24.9/102.5 MB 4.6 MB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 25.7/102.5 MB 4.5 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 26.2/102.5 MB 4.5 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 27.0/102.5 MB 4.4 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 28.0/102.5 MB 4.4 MB/s eta 0:00:17\n",
      "   ----------- ---------------------------- 29.1/102.5 MB 4.5 MB/s eta 0:00:17\n",
      "   ----------- ---------------------------- 30.4/102.5 MB 4.5 MB/s eta 0:00:16\n",
      "   ------------ --------------------------- 31.5/102.5 MB 4.5 MB/s eta 0:00:16\n",
      "   ------------ --------------------------- 32.2/102.5 MB 4.5 MB/s eta 0:00:16\n",
      "   ------------ --------------------------- 33.0/102.5 MB 4.5 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 34.1/102.5 MB 4.5 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 35.1/102.5 MB 4.5 MB/s eta 0:00:15\n",
      "   -------------- ------------------------- 36.2/102.5 MB 4.5 MB/s eta 0:00:15\n",
      "   -------------- ------------------------- 37.2/102.5 MB 4.5 MB/s eta 0:00:15\n",
      "   -------------- ------------------------- 38.0/102.5 MB 4.5 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 39.1/102.5 MB 4.5 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 39.8/102.5 MB 4.5 MB/s eta 0:00:14\n",
      "   --------------- ------------------------ 40.6/102.5 MB 4.5 MB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 41.4/102.5 MB 4.5 MB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 42.2/102.5 MB 4.5 MB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 43.0/102.5 MB 4.4 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 43.8/102.5 MB 4.4 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 44.6/102.5 MB 4.4 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 45.1/102.5 MB 4.4 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 46.1/102.5 MB 4.4 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 46.7/102.5 MB 4.3 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 47.4/102.5 MB 4.3 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 48.5/102.5 MB 4.3 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 49.0/102.5 MB 4.3 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 50.3/102.5 MB 4.3 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 51.6/102.5 MB 4.4 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 52.4/102.5 MB 4.4 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 53.2/102.5 MB 4.3 MB/s eta 0:00:12\n",
      "   --------------------- ------------------ 54.3/102.5 MB 4.4 MB/s eta 0:00:12\n",
      "   --------------------- ------------------ 55.3/102.5 MB 4.4 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 56.1/102.5 MB 4.4 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 57.1/102.5 MB 4.4 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 58.2/102.5 MB 4.4 MB/s eta 0:00:11\n",
      "   ----------------------- ---------------- 59.0/102.5 MB 4.4 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 59.8/102.5 MB 4.4 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 60.6/102.5 MB 4.3 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 61.6/102.5 MB 4.3 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 62.1/102.5 MB 4.3 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 62.9/102.5 MB 4.3 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 63.7/102.5 MB 4.3 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 64.7/102.5 MB 4.3 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 65.8/102.5 MB 4.3 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 66.8/102.5 MB 4.3 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 67.6/102.5 MB 4.3 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 68.7/102.5 MB 4.3 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 70.0/102.5 MB 4.3 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 71.0/102.5 MB 4.4 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 72.1/102.5 MB 4.4 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 73.4/102.5 MB 4.4 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 74.2/102.5 MB 4.4 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 75.2/102.5 MB 4.4 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 76.3/102.5 MB 4.4 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 77.3/102.5 MB 4.4 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 78.1/102.5 MB 4.4 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 79.2/102.5 MB 4.4 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 80.2/102.5 MB 4.4 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 81.3/102.5 MB 4.4 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 82.1/102.5 MB 4.4 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 83.6/102.5 MB 4.4 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 84.7/102.5 MB 4.4 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 85.5/102.5 MB 4.4 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 86.5/102.5 MB 4.4 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 87.8/102.5 MB 4.5 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 88.6/102.5 MB 4.5 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 89.7/102.5 MB 4.5 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 90.2/102.5 MB 4.5 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 91.0/102.5 MB 4.4 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 91.8/102.5 MB 4.4 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 92.5/102.5 MB 4.4 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 93.3/102.5 MB 4.4 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 93.8/102.5 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 94.9/102.5 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 95.7/102.5 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 96.5/102.5 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 97.3/102.5 MB 4.4 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 98.0/102.5 MB 4.4 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 98.3/102.5 MB 4.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 98.6/102.5 MB 4.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 98.8/102.5 MB 4.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 99.6/102.5 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  100.7/102.5 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  101.7/102.5 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  102.2/102.5 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 102.5/102.5 MB 4.3 MB/s  0:00:24\n",
      "Downloading graphviz-0.21-py3-none-any.whl (47 kB)\n",
      "Downloading plotly-6.3.0-py3-none-any.whl (9.8 MB)\n",
      "   ---------------------------------------- 0.0/9.8 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.8/9.8 MB 4.8 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.8/9.8 MB 4.4 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.6/9.8 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.4/9.8 MB 4.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.2/9.8 MB 4.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.0/9.8 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.3/9.8 MB 4.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.3/9.8 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.4/9.8 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.7/9.8 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.8/9.8 MB 4.6 MB/s  0:00:02\n",
      "Downloading narwhals-2.1.1-py3-none-any.whl (389 kB)\n",
      "Installing collected packages: narwhals, graphviz, plotly, catboost\n",
      "\n",
      "   ---------------------------------------- 0/4 [narwhals]\n",
      "   ---------------------------------------- 0/4 [narwhals]\n",
      "   ---------------------------------------- 0/4 [narwhals]\n",
      "   ---------------------------------------- 0/4 [narwhals]\n",
      "   ---------------------------------------- 0/4 [narwhals]\n",
      "   ---------------------------------------- 0/4 [narwhals]\n",
      "   ---------------------------------------- 0/4 [narwhals]\n",
      "   ---------------------------------------- 0/4 [narwhals]\n",
      "   ---------------------------------------- 0/4 [narwhals]\n",
      "   ---------------------------------------- 0/4 [narwhals]\n",
      "   ---------------------------------------- 0/4 [narwhals]\n",
      "   ---------- ----------------------------- 1/4 [graphviz]\n",
      "   ---------- ----------------------------- 1/4 [graphviz]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   ------------------------------ --------- 3/4 [catboost]\n",
      "   ------------------------------ --------- 3/4 [catboost]\n",
      "   ------------------------------ --------- 3/4 [catboost]\n",
      "   ------------------------------ --------- 3/4 [catboost]\n",
      "   ------------------------------ --------- 3/4 [catboost]\n",
      "   ------------------------------ --------- 3/4 [catboost]\n",
      "   ------------------------------ --------- 3/4 [catboost]\n",
      "   ------------------------------ --------- 3/4 [catboost]\n",
      "   ------------------------------ --------- 3/4 [catboost]\n",
      "   ------------------------------ --------- 3/4 [catboost]\n",
      "   ------------------------------ --------- 3/4 [catboost]\n",
      "   ---------------------------------------- 4/4 [catboost]\n",
      "\n",
      "Successfully installed catboost-1.2.8 graphviz-0.21 narwhals-2.1.1 plotly-6.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f0b2592-7ba8-42fc-92f4-2e1496e6e07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BTC', 'Clusters', 'ExpAddress', 'Family', 'Flag', 'IPAddress', 'Netflow_Bytes', 'Port', 'Prediction', 'Protocol', 'SeedAddress', 'Threats', 'Time', 'USD']\n"
     ]
    }
   ],
   "source": [
    "# --- Normalize behavioral column names to what our code expects ---\n",
    "rename_map = {\n",
    "    \"Protcol\": \"Protocol\",        # typo in CSV\n",
    "    \"SeddAddress\": \"SeedAddress\", # typo in CSV\n",
    "    \"IPaddress\": \"IPAddress\"      # capitalization\n",
    "    # \"Prediction\" exists but we won't use it\n",
    "}\n",
    "\n",
    "df_behav = df_behav.rename(columns=rename_map)\n",
    "\n",
    "# quick sanity check\n",
    "print(sorted(df_behav.columns.tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9687de59-6623-47ed-9c2b-e052a1f75fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: []\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Flag</th>\n",
       "      <th>Threats</th>\n",
       "      <th>ProtoFlag</th>\n",
       "      <th>PortBucket</th>\n",
       "      <th>has_BTC</th>\n",
       "      <th>has_USD</th>\n",
       "      <th>Time</th>\n",
       "      <th>Netflow_Bytes</th>\n",
       "      <th>BTC</th>\n",
       "      <th>USD</th>\n",
       "      <th>Clusters</th>\n",
       "      <th>is_wannacry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCP</td>\n",
       "      <td>A</td>\n",
       "      <td>Bonet</td>\n",
       "      <td>TCP_A</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>6.216606</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCP</td>\n",
       "      <td>A</td>\n",
       "      <td>Bonet</td>\n",
       "      <td>TCP_A</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>6.224558</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TCP</td>\n",
       "      <td>A</td>\n",
       "      <td>Bonet</td>\n",
       "      <td>TCP_A</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>6.232448</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TCP</td>\n",
       "      <td>A</td>\n",
       "      <td>Bonet</td>\n",
       "      <td>TCP_A</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>2.772589</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>6.240276</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCP</td>\n",
       "      <td>A</td>\n",
       "      <td>Bonet</td>\n",
       "      <td>TCP_A</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>6.248043</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Protocol Flag Threats ProtoFlag  PortBucket  has_BTC  has_USD  Time  \\\n",
       "0      TCP    A   Bonet     TCP_A       10000        1        1    50   \n",
       "1      TCP    A   Bonet     TCP_A       10000        1        1    40   \n",
       "2      TCP    A   Bonet     TCP_A       10000        1        1    30   \n",
       "3      TCP    A   Bonet     TCP_A       10000        1        1    20   \n",
       "4      TCP    A   Bonet     TCP_A       10000        1        1    57   \n",
       "\n",
       "   Netflow_Bytes       BTC       USD  Clusters  is_wannacry  \n",
       "0       1.791759  0.693147  6.216606         1            1  \n",
       "1       2.197225  0.693147  6.224558         1            1  \n",
       "2       2.079442  0.693147  6.232448         1            1  \n",
       "3       2.772589  0.693147  6.240276         1            1  \n",
       "4       2.302585  0.693147  6.248043         1            1  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_b = df_behav.copy()\n",
    "\n",
    "# Target: WannaCry vs not\n",
    "df_b[\"is_wannacry\"] = (df_b[\"Family\"].astype(str) == \"WannaCry\").astype(int)\n",
    "\n",
    "# Feature engineering used by the CatBoost approach\n",
    "# (these names are what the CatBoost code expects)\n",
    "df_b[\"ProtoFlag\"]  = df_b[\"Protocol\"].astype(str) + \"_\" + df_b[\"Flag\"].astype(str)\n",
    "\n",
    "def bucket_port(p):\n",
    "    if pd.isna(p): return -1\n",
    "    try:\n",
    "        p = int(p)\n",
    "    except Exception:\n",
    "        return -1\n",
    "    if p in {80, 443, 53, 25, 110, 143, 995, 993}: return p\n",
    "    if p < 1024:   return 1000\n",
    "    if p < 10000:  return 10000\n",
    "    return 65535\n",
    "\n",
    "df_b[\"PortBucket\"] = df_b[\"Port\"].apply(bucket_port)\n",
    "df_b[\"has_BTC\"]    = (pd.to_numeric(df_b[\"BTC\"], errors=\"coerce\").fillna(0) > 0).astype(int)\n",
    "df_b[\"has_USD\"]    = (pd.to_numeric(df_b[\"USD\"], errors=\"coerce\").fillna(0) > 0).astype(int)\n",
    "\n",
    "# Log transforms for skewed numerics\n",
    "for c in [\"BTC\",\"USD\",\"Netflow_Bytes\"]:\n",
    "    if c in df_b.columns:\n",
    "        df_b[c] = np.log1p(pd.to_numeric(df_b[c], errors=\"coerce\").fillna(0))\n",
    "\n",
    "# (Optional) ensure numeric dtypes for the core numeric set\n",
    "for c in [\"Time\",\"Clusters\",\"Port\"]:\n",
    "    if c in df_b.columns:\n",
    "        df_b[c] = pd.to_numeric(df_b[c], errors=\"coerce\")\n",
    "\n",
    "# final quick check (these should all exist now)\n",
    "needed = [\n",
    "    \"Protocol\",\"Flag\",\"Threats\",\"ProtoFlag\",\"PortBucket\",\"has_BTC\",\"has_USD\",\n",
    "    \"Time\",\"Netflow_Bytes\",\"BTC\",\"USD\",\"Clusters\",\"is_wannacry\"\n",
    "]\n",
    "missing = [c for c in needed if c not in df_b.columns]\n",
    "print(\"Missing:\", missing)  # should be []\n",
    "df_b[needed].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4462c64a-e0eb-46ff-b8cb-c45ae464e005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen threshold (max F1): 0.671\n",
      "[[19814  6773]\n",
      " [ 1169  2053]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.944     0.745     0.833     26587\n",
      "           1      0.233     0.637     0.341      3222\n",
      "\n",
      "    accuracy                          0.734     29809\n",
      "   macro avg      0.588     0.691     0.587     29809\n",
      "weighted avg      0.867     0.734     0.780     29809\n",
      "\n",
      "Saved: behav_catboost.cbm behav_catboost_schema.json\n"
     ]
    }
   ],
   "source": [
    "# === BEHAVIORAL with CatBoost (replaces XGB for behavioral only) ===\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, classification_report, confusion_matrix\n",
    "import numpy as np, json\n",
    "from pathlib import Path\n",
    "\n",
    "# Categorical and numerical columns\n",
    "cat_cols = [\"Protocol\",\"Flag\",\"Threats\",\"ProtoFlag\",\"PortBucket\",\"has_BTC\",\"has_USD\"]\n",
    "num_cols = [\"Time\",\"Netflow_Bytes\",\"BTC\",\"USD\",\"Clusters\"]\n",
    "use_cols = num_cols + cat_cols\n",
    "\n",
    "# Data and target\n",
    "X_cb = df_b[use_cols].copy()\n",
    "y_cb = df_b[\"is_wannacry\"].copy()\n",
    "\n",
    "# Train-test split\n",
    "Xtr_cb, Xte_cb, ytr_cb, yte_cb = train_test_split(\n",
    "    X_cb, y_cb, test_size=0.20, stratify=y_cb, random_state=42\n",
    ")\n",
    "\n",
    "# Indices of categorical columns\n",
    "cat_idx = [X_cb.columns.get_loc(c) for c in cat_cols]\n",
    "\n",
    "# Create CatBoost pools\n",
    "train_pool = Pool(Xtr_cb, ytr_cb, cat_features=cat_idx)\n",
    "val_pool   = Pool(Xte_cb, yte_cb, cat_features=cat_idx)\n",
    "\n",
    "# Handle class imbalance\n",
    "neg = int((ytr_cb == 0).sum())\n",
    "pos = int((ytr_cb == 1).sum())\n",
    "spw = neg / max(1, pos)\n",
    "\n",
    "# Train CatBoost\n",
    "cb = CatBoostClassifier(\n",
    "    iterations=1200,\n",
    "    depth=6,\n",
    "    learning_rate=0.05,\n",
    "    loss_function=\"Logloss\",\n",
    "    eval_metric=\"PRAUC\",\n",
    "    scale_pos_weight=spw * 1.2,\n",
    "    random_seed=42,\n",
    "    verbose=False,\n",
    "    od_type=\"Iter\",\n",
    "    od_wait=50,      # stop if no val improvement for 50 rounds\n",
    ")\n",
    "cb.fit(train_pool, eval_set=val_pool, use_best_model=True)\n",
    "\n",
    "# Threshold tuning\n",
    "proba_cb = cb.predict_proba(val_pool)[:, 1]\n",
    "prec, rec, thr = precision_recall_curve(yte_cb, proba_cb)\n",
    "f1 = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "best_idx = int(np.nanargmax(f1))\n",
    "best_thr_cb = float(thr[best_idx]) if best_idx < len(thr) else 0.5\n",
    "\n",
    "print(f\"Chosen threshold (max F1): {best_thr_cb:.3f}\")\n",
    "pred_cb = (proba_cb >= best_thr_cb).astype(int)\n",
    "print(confusion_matrix(yte_cb, pred_cb))\n",
    "print(classification_report(yte_cb, pred_cb, digits=3))\n",
    "\n",
    "# Save model and schema\n",
    "MODEL_DIR = Path(\"models\"); MODEL_DIR.mkdir(exist_ok=True)\n",
    "cb.save_model(MODEL_DIR / \"behav_catboost.cbm\")\n",
    "with open(MODEL_DIR / \"behav_catboost_schema.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"use_cols\": use_cols,\n",
    "        \"cat_cols\": cat_cols,\n",
    "        \"num_cols\": num_cols,\n",
    "        \"threshold\": best_thr_cb\n",
    "    }, f)\n",
    "\n",
    "print(\"Saved:\", \"behav_catboost.cbm\", \"behav_catboost_schema.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ac4a09d-ad5a-476f-afc1-4ed081edbd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 predictions: [1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# === Quick inference test on same df_b ===\n",
    "from catboost import CatBoostClassifier\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_DIR = Path(\"models\")  # adjust if your files are in another folder\n",
    "cb = CatBoostClassifier()\n",
    "cb.load_model(MODEL_DIR / \"behav_catboost.cbm\")\n",
    "\n",
    "schema = json.loads((MODEL_DIR / \"behav_catboost_schema.json\").read_text())\n",
    "\n",
    "# Just test on the first 5 rows of your current df_b\n",
    "proba = cb.predict_proba(df_b[schema[\"use_cols\"]])[:, 1]\n",
    "pred = (proba >= schema[\"threshold\"]).astype(int)\n",
    "print(\"First 5 predictions:\", pred[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e350c06-b34e-4fe7-800d-1c4004b5d135",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Predict again on validation set\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m proba_cb = \u001b[43mcb\u001b[49m.predict_proba(val_pool)[:, \u001b[32m1\u001b[39m]\n\u001b[32m      5\u001b[39m pred_cb = (proba_cb >= best_thr_cb).astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Show confusion matrix and classification report\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'cb' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict again on validation set\n",
    "proba_cb = cb.predict_proba(val_pool)[:, 1]\n",
    "pred_cb = (proba_cb >= best_thr_cb).astype(int)\n",
    "\n",
    "# Show confusion matrix and classification report\n",
    "print(confusion_matrix(yte_cb, pred_cb))\n",
    "print(classification_report(yte_cb, pred_cb, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bf7a861-f2b8-4b67-9435-d58146f4b487",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m proba_cb = \u001b[43mcb\u001b[49m.predict_proba(val_pool)[:, \u001b[32m1\u001b[39m]\n\u001b[32m      4\u001b[39m pred_cb  = (proba_cb >= best_thr_cb).astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(confusion_matrix(yte_cb, pred_cb))\n",
      "\u001b[31mNameError\u001b[39m: name 'cb' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "proba_cb = cb.predict_proba(val_pool)[:, 1]\n",
    "pred_cb  = (proba_cb >= best_thr_cb).astype(int)\n",
    "\n",
    "print(confusion_matrix(yte_cb, pred_cb))\n",
    "print(classification_report(yte_cb, pred_cb, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "994e6657-d1a2-4e76-828b-c4a6a0e270df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m proba_cb = \u001b[43mcb\u001b[49m.predict_proba(val_pool)[:, \u001b[32m1\u001b[39m]\n\u001b[32m      4\u001b[39m pred_cb  = (proba_cb >= best_thr_cb).astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(confusion_matrix(yte_cb, pred_cb))\n",
      "\u001b[31mNameError\u001b[39m: name 'cb' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "proba_cb = cb.predict_proba(val_pool)[:, 1]\n",
    "pred_cb  = (proba_cb >= best_thr_cb).astype(int)\n",
    "\n",
    "print(confusion_matrix(yte_cb, pred_cb))\n",
    "print(classification_report(yte_cb, pred_cb, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1837b07-b6da-49a5-b036-d093cffeaabe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m num_cols = [\u001b[33m\"\u001b[39m\u001b[33mTime\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mNetflow_Bytes\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mBTC\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mUSD\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mClusters\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     10\u001b[39m use_cols = num_cols + cat_cols\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m X_cb = \u001b[43mdf_b\u001b[49m[use_cols].copy()\n\u001b[32m     13\u001b[39m y_cb = df_b[\u001b[33m\"\u001b[39m\u001b[33mis_wannacry\u001b[39m\u001b[33m\"\u001b[39m].copy()\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# 2) Same split as training\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df_b' is not defined"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Rebuild features the same way as before (df_b already exists from earlier cell)\n",
    "cat_cols = [\"Protocol\",\"Flag\",\"Threats\",\"ProtoFlag\",\"PortBucket\",\"has_BTC\",\"has_USD\"]\n",
    "num_cols = [\"Time\",\"Netflow_Bytes\",\"BTC\",\"USD\",\"Clusters\"]\n",
    "use_cols = num_cols + cat_cols\n",
    "\n",
    "X_cb = df_b[use_cols].copy()\n",
    "y_cb = df_b[\"is_wannacry\"].copy()\n",
    "\n",
    "# 2) Same split as training\n",
    "Xtr_cb, Xte_cb, ytr_cb, yte_cb = train_test_split(\n",
    "    X_cb, y_cb, test_size=0.20, stratify=y_cb, random_state=42\n",
    ")\n",
    "\n",
    "# 3) Make CatBoost Pool for the validation set\n",
    "cat_idx  = [X_cb.columns.get_loc(c) for c in cat_cols]\n",
    "val_pool = Pool(Xte_cb, yte_cb, cat_features=cat_idx)\n",
    "\n",
    "# 4) Load model and threshold\n",
    "MODEL_DIR = Path(\"models\")\n",
    "cb = CatBoostClassifier()\n",
    "cb.load_model(MODEL_DIR / \"behav_catboost.cbm\")\n",
    "schema = json.loads((MODEL_DIR / \"behav_catboost_schema.json\").read_text())\n",
    "best_thr_cb = float(schema[\"threshold\"])\n",
    "\n",
    "# 5) Recompute metrics\n",
    "proba_cb = cb.predict_proba(val_pool)[:, 1]\n",
    "pred_cb  = (proba_cb >= best_thr_cb).astype(int)\n",
    "\n",
    "print(confusion_matrix(yte_cb, pred_cb))\n",
    "print(classification_report(yte_cb, pred_cb, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5962dd48-7a41-48af-87a3-f1199d2e06c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets\\\\behavioral\\\\ugransom.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m MODEL_DIR = Path(\u001b[33m\"\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 1) Load CSV and fix header typos IN MEMORY (we do NOT modify the file)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m df_behav = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m rename_map = {\n\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mProtcol\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mProtocol\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mSeddAddress\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mSeedAddress\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mIPaddress\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mIPAddress\u001b[39m\u001b[33m\"\u001b[39m,   \u001b[38;5;66;03m# we don't use it, but normalize anyway\u001b[39;00m\n\u001b[32m     21\u001b[39m }\n\u001b[32m     22\u001b[39m df_behav = df_behav.rename(columns=rename_map)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'datasets\\\\behavioral\\\\ugransom.csv'"
     ]
    }
   ],
   "source": [
    "# === Recreate state after a restart & re-evaluate CatBoost on validation ===\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# 0) Paths\n",
    "DATA = Path(\"datasets/behavioral/ugransom.csv\")\n",
    "MODEL_DIR = Path(\"models\")\n",
    "\n",
    "# 1) Load CSV and fix header typos IN MEMORY (we do NOT modify the file)\n",
    "df_behav = pd.read_csv(DATA)\n",
    "\n",
    "rename_map = {\n",
    "    \"Protcol\": \"Protocol\",\n",
    "    \"SeddAddress\": \"SeedAddress\",\n",
    "    \"IPaddress\": \"IPAddress\",   # we don't use it, but normalize anyway\n",
    "}\n",
    "df_behav = df_behav.rename(columns=rename_map)\n",
    "\n",
    "# 2) Rebuild engineered behavioral frame df_b (same as when you trained)\n",
    "df_b = df_behav.copy()\n",
    "\n",
    "# target\n",
    "df_b[\"is_wannacry\"] = (df_b[\"Family\"].astype(str) == \"WannaCry\").astype(int)\n",
    "\n",
    "# features expected by CatBoost version\n",
    "df_b[\"ProtoFlag\"] = df_b[\"Protocol\"].astype(str) + \"_\" + df_b[\"Flag\"].astype(str)\n",
    "\n",
    "def bucket_port(p):\n",
    "    if pd.isna(p): return -1\n",
    "    try:\n",
    "        p = int(p)\n",
    "    except Exception:\n",
    "        return -1\n",
    "    if p in {80, 443, 53, 25, 110, 143, 995, 993}: return p\n",
    "    if p < 1024:   return 1000\n",
    "    if p < 10000:  return 10000\n",
    "    return 65535\n",
    "\n",
    "df_b[\"PortBucket\"] = df_b[\"Port\"].apply(bucket_port)\n",
    "df_b[\"has_BTC\"]    = (pd.to_numeric(df_b[\"BTC\"], errors=\"coerce\").fillna(0) > 0).astype(int)\n",
    "df_b[\"has_USD\"]    = (pd.to_numeric(df_b[\"USD\"], errors=\"coerce\").fillna(0) > 0).astype(int)\n",
    "\n",
    "for c in [\"BTC\",\"USD\",\"Netflow_Bytes\"]:\n",
    "    if c in df_b.columns:\n",
    "        df_b[c] = np.log1p(pd.to_numeric(df_b[c], errors=\"coerce\").fillna(0))\n",
    "for c in [\"Time\",\"Clusters\",\"Port\"]:\n",
    "    if c in df_b.columns:\n",
    "        df_b[c] = pd.to_numeric(df_b[c], errors=\"coerce\")\n",
    "\n",
    "# 3) Load schema & model you saved last time\n",
    "schema = json.loads((MODEL_DIR / \"behav_catboost_schema.json\").read_text())\n",
    "use_cols = schema[\"use_cols\"]\n",
    "cat_cols = schema[\"cat_cols\"]\n",
    "num_cols = schema[\"num_cols\"]\n",
    "thr_cb   = float(schema[\"threshold\"])\n",
    "\n",
    "# sanity check: make sure we have the columns we expect\n",
    "missing = [c for c in use_cols if c not in df_b.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected columns in df_b: {missing}\")\n",
    "\n",
    "# 4) Recreate the SAME split & CatBoost Pool as training used\n",
    "X_cb = df_b[use_cols].copy()\n",
    "y_cb = df_b[\"is_wannacry\"].copy()\n",
    "\n",
    "Xtr_cb, Xte_cb, ytr_cb, yte_cb = train_test_split(\n",
    "    X_cb, y_cb, test_size=0.20, stratify=y_cb, random_state=42\n",
    ")\n",
    "\n",
    "cat_idx = [use_cols.index(c) for c in cat_cols]\n",
    "val_pool = Pool(Xte_cb, yte_cb, cat_features=cat_idx)\n",
    "\n",
    "# 5) Load the trained CatBoost model and evaluate on the validation set\n",
    "cb = CatBoostClassifier()\n",
    "cb.load_model(MODEL_DIR / \"behav_catboost.cbm\")\n",
    "\n",
    "proba_cb = cb.predict_proba(val_pool)[:, 1]\n",
    "pred_cb  = (proba_cb >= thr_cb).astype(int)\n",
    "\n",
    "print(\"=== CATBOOST (WannaCry vs Not)  Validation re-check ===\")\n",
    "print(confusion_matrix(yte_cb, pred_cb))\n",
    "print(classification_report(yte_cb, pred_cb, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05311016-1f2d-4689-8c5c-f050cf417777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CATBOOST (WannaCry vs Not)  Validation re-check ===\n",
      "[[19814  6773]\n",
      " [ 1169  2053]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.944     0.745     0.833     26587\n",
      "           1      0.233     0.637     0.341      3222\n",
      "\n",
      "    accuracy                          0.734     29809\n",
      "   macro avg      0.588     0.691     0.587     29809\n",
      "weighted avg      0.867     0.734     0.780     29809\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === Recreate state after a restart & re-evaluate CatBoost on validation ===\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# 0) Paths\n",
    "DATA = Path(r\"C:/Users/richa/OneDrive/Documents/FYP2/datasets/behavioral/ugransom.csv\")\n",
    "MODEL_DIR = Path(\"models\")\n",
    "\n",
    "# 1) Load CSV and fix header typos IN MEMORY (we do NOT modify the file)\n",
    "df_behav = pd.read_csv(DATA)\n",
    "\n",
    "rename_map = {\n",
    "    \"Protcol\": \"Protocol\",\n",
    "    \"SeddAddress\": \"SeedAddress\",\n",
    "    \"IPaddress\": \"IPAddress\",   # we don't use it, but normalize anyway\n",
    "}\n",
    "df_behav = df_behav.rename(columns=rename_map)\n",
    "\n",
    "# 2) Rebuild engineered behavioral frame df_b (same as when you trained)\n",
    "df_b = df_behav.copy()\n",
    "\n",
    "# target\n",
    "df_b[\"is_wannacry\"] = (df_b[\"Family\"].astype(str) == \"WannaCry\").astype(int)\n",
    "\n",
    "# features expected by CatBoost version\n",
    "df_b[\"ProtoFlag\"] = df_b[\"Protocol\"].astype(str) + \"_\" + df_b[\"Flag\"].astype(str)\n",
    "\n",
    "def bucket_port(p):\n",
    "    if pd.isna(p): return -1\n",
    "    try:\n",
    "        p = int(p)\n",
    "    except Exception:\n",
    "        return -1\n",
    "    if p in {80, 443, 53, 25, 110, 143, 995, 993}: return p\n",
    "    if p < 1024:   return 1000\n",
    "    if p < 10000:  return 10000\n",
    "    return 65535\n",
    "\n",
    "df_b[\"PortBucket\"] = df_b[\"Port\"].apply(bucket_port)\n",
    "df_b[\"has_BTC\"]    = (pd.to_numeric(df_b[\"BTC\"], errors=\"coerce\").fillna(0) > 0).astype(int)\n",
    "df_b[\"has_USD\"]    = (pd.to_numeric(df_b[\"USD\"], errors=\"coerce\").fillna(0) > 0).astype(int)\n",
    "\n",
    "for c in [\"BTC\",\"USD\",\"Netflow_Bytes\"]:\n",
    "    if c in df_b.columns:\n",
    "        df_b[c] = np.log1p(pd.to_numeric(df_b[c], errors=\"coerce\").fillna(0))\n",
    "for c in [\"Time\",\"Clusters\",\"Port\"]:\n",
    "    if c in df_b.columns:\n",
    "        df_b[c] = pd.to_numeric(df_b[c], errors=\"coerce\")\n",
    "\n",
    "# 3) Load schema & model you saved last time\n",
    "schema = json.loads((MODEL_DIR / \"behav_catboost_schema.json\").read_text())\n",
    "use_cols = schema[\"use_cols\"]\n",
    "cat_cols = schema[\"cat_cols\"]\n",
    "num_cols = schema[\"num_cols\"]\n",
    "thr_cb   = float(schema[\"threshold\"])\n",
    "\n",
    "# sanity check: make sure we have the columns we expect\n",
    "missing = [c for c in use_cols if c not in df_b.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected columns in df_b: {missing}\")\n",
    "\n",
    "# 4) Recreate the SAME split & CatBoost Pool as training used\n",
    "X_cb = df_b[use_cols].copy()\n",
    "y_cb = df_b[\"is_wannacry\"].copy()\n",
    "\n",
    "Xtr_cb, Xte_cb, ytr_cb, yte_cb = train_test_split(\n",
    "    X_cb, y_cb, test_size=0.20, stratify=y_cb, random_state=42\n",
    ")\n",
    "\n",
    "cat_idx = [use_cols.index(c) for c in cat_cols]\n",
    "val_pool = Pool(Xte_cb, yte_cb, cat_features=cat_idx)\n",
    "\n",
    "# 5) Load the trained CatBoost model and evaluate on the validation set\n",
    "cb = CatBoostClassifier()\n",
    "cb.load_model(MODEL_DIR / \"behav_catboost.cbm\")\n",
    "\n",
    "proba_cb = cb.predict_proba(val_pool)[:, 1]\n",
    "pred_cb  = (proba_cb >= thr_cb).astype(int)\n",
    "\n",
    "print(\"=== CATBOOST (WannaCry vs Not)  Validation re-check ===\")\n",
    "print(confusion_matrix(yte_cb, pred_cb))\n",
    "print(classification_report(yte_cb, pred_cb, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7599f5-4849-4f6d-b465-a30d06a12205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
