{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c03a63e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# let notebook access code in src/\n",
    "project_root = Path().resolve().parent  # .../FYP2\n",
    "sys.path.append(str(project_root / \"src\"))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# for Static model candidates\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except ImportError:\n",
    "    print(\"If xgboost import fails, install it in your venv: pip install xgboost\")\n",
    "\n",
    "# for Behavioral model\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# for dealing with imbalance\n",
    "from sklearn.utils.class_weight import compute_class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faf04d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATIC SHAPE: (62485, 15) (62485,)\n",
      "STATIC class distribution: [27118 35367]\n",
      "BEHAV SHAPE (before col filtering): (149043, 19) (149043,)\n",
      "BEHAV class distribution: [106482  42561]\n",
      "BEHAV SHAPE (numeric only): (149043, 10)\n"
     ]
    }
   ],
   "source": [
    "# ---------- Load STATIC data (file-based features like PE headers) ----------\n",
    "\n",
    "df_static = pd.read_parquet(str(project_root / \"data_processed/static_baseline.parquet\"))\n",
    "\n",
    "# Ground truth label: 1 = malicious (WannaCry), 0 = benign\n",
    "y_static = (df_static[\"Benign\"] == 0).astype(int)\n",
    "\n",
    "# Drop non-feature columns that shouldn't go into training\n",
    "# Keep only numeric PE-related features\n",
    "# We'll guess for now that Feature columns = all columns except ['FileName','md5Hash','Benign']\n",
    "static_drop_cols = [\"FileName\", \"md5Hash\", \"Benign\"]\n",
    "X_static = df_static.drop(columns=static_drop_cols, errors=\"ignore\")\n",
    "\n",
    "print(\"STATIC SHAPE:\", X_static.shape, y_static.shape)\n",
    "print(\"STATIC class distribution:\", np.bincount(y_static))\n",
    "\n",
    "\n",
    "# ---------- Load BEHAVIORAL data (network / process-like features) ----------\n",
    "\n",
    "df_behav = pd.read_parquet(str(project_root / \"data_processed/behav_baseline.parquet\"))\n",
    "\n",
    "# Ground truth label: from 'Prediction' where 'A' means WannaCry\n",
    "y_behav = df_behav[\"Prediction\"].apply(lambda x: 1 if x == \"A\" else 0).astype(int)\n",
    "\n",
    "# For behavioral features:\n",
    "# We will reuse the raw columns and generate engineered features similar to build_behav_features().\n",
    "# Since build_behav_features lives in src/utils.py, let's import and call it.\n",
    "\n",
    "from utils import build_behav_features\n",
    "X_behav_full = build_behav_features(df_behav)\n",
    "\n",
    "print(\"BEHAV SHAPE (before col filtering):\", X_behav_full.shape, y_behav.shape)\n",
    "print(\"BEHAV class distribution:\", np.bincount(y_behav))\n",
    "\n",
    "# Now: we will drop columns that are obviously IDs / text addresses if they make the model confused.\n",
    "# Optionally keep numeric / engineered columns only.\n",
    "# We'll do a simple filter: keep numeric columns.\n",
    "X_behav = X_behav_full.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "print(\"BEHAV SHAPE (numeric only):\", X_behav.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dc34a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Behavioral train size: (119234, 10) test size: (29809, 10)\n",
      "Behavioral class weights: {np.int64(0): np.float64(0.6998532605505664), np.int64(1): np.float64(1.7509177949425827)}\n",
      "=== Tuned Behavioral CatBoost ===\n",
      "Best threshold  : 0.620\n",
      "Accuracy        : 0.9831\n",
      "Precision       : 0.9791\n",
      "Recall (TPR)    : 0.9612\n",
      "F1-score        : 0.9701\n",
      "ROC AUC         : 0.9974\n",
      "False Pos Rate  : 0.0082\n",
      "Confusion Matrix [tn fp; fn tp]:\n",
      "[[np.int64(21122), np.int64(175)], [np.int64(330), np.int64(8182)]]\n"
     ]
    }
   ],
   "source": [
    "# ---------- Train/Test split for BEHAVIORAL ----------\n",
    "Xb_train, Xb_test, yb_train, yb_test = train_test_split(\n",
    "    X_behav, y_behav,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_behav\n",
    ")\n",
    "\n",
    "print(\"Behavioral train size:\", Xb_train.shape, \"test size:\", Xb_test.shape)\n",
    "\n",
    "# Compute class weights so model pays more attention to WannaCry class (1)\n",
    "classes = np.unique(yb_train)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=classes,\n",
    "    y=yb_train\n",
    ")\n",
    "class_weights_dict = {cls: w for cls, w in zip(classes, class_weights)}\n",
    "print(\"Behavioral class weights:\", class_weights_dict)\n",
    "\n",
    "# Prepare CatBoost Pools (this is CatBoost's native data wrapper)\n",
    "train_pool = Pool(Xb_train, label=yb_train)\n",
    "test_pool  = Pool(Xb_test,  label=yb_test)\n",
    "\n",
    "# Train a tuned CatBoost\n",
    "cat_model = CatBoostClassifier(\n",
    "    iterations=800,\n",
    "    depth=8,\n",
    "    learning_rate=0.05,\n",
    "    l2_leaf_reg=3,\n",
    "    loss_function=\"Logloss\",\n",
    "    eval_metric=\"AUC\",\n",
    "    verbose=False,\n",
    "    class_weights=[class_weights_dict.get(0,1.0), class_weights_dict.get(1,1.0)],\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "cat_model.fit(train_pool, eval_set=test_pool)\n",
    "\n",
    "# Predict probabilities and classes\n",
    "proba_b = cat_model.predict_proba(Xb_test)[:, 1]\n",
    "\n",
    "# Instead of a hardcoded threshold, let's search best F1 threshold\n",
    "thresholds = np.linspace(0.1, 0.9, 81)\n",
    "best_f1 = -1\n",
    "best_thr = 0.5\n",
    "for t in thresholds:\n",
    "    preds_t = (proba_b >= t).astype(int)\n",
    "    f1_t = f1_score(yb_test, preds_t, zero_division=0)\n",
    "    if f1_t > best_f1:\n",
    "        best_f1 = f1_t\n",
    "        best_thr = t\n",
    "\n",
    "preds_b = (proba_b >= best_thr).astype(int)\n",
    "\n",
    "# Evaluate tuned behavioral model\n",
    "acc_b  = accuracy_score(yb_test, preds_b)\n",
    "prec_b = precision_score(yb_test, preds_b, zero_division=0)\n",
    "rec_b  = recall_score(yb_test, preds_b, zero_division=0)\n",
    "f1_b   = f1_score(yb_test, preds_b, zero_division=0)\n",
    "auc_b  = roc_auc_score(yb_test, proba_b)\n",
    "tn_b, fp_b, fn_b, tp_b = confusion_matrix(yb_test, preds_b).ravel()\n",
    "fpr_b = fp_b / (fp_b + tn_b) if (fp_b + tn_b) > 0 else 0.0\n",
    "\n",
    "print(\"=== Tuned Behavioral CatBoost ===\")\n",
    "print(f\"Best threshold  : {best_thr:.3f}\")\n",
    "print(f\"Accuracy        : {acc_b:.4f}\")\n",
    "print(f\"Precision       : {prec_b:.4f}\")\n",
    "print(f\"Recall (TPR)    : {rec_b:.4f}\")\n",
    "print(f\"F1-score        : {f1_b:.4f}\")\n",
    "print(f\"ROC AUC         : {auc_b:.4f}\")\n",
    "print(f\"False Pos Rate  : {fpr_b:.4f}\")\n",
    "print(\"Confusion Matrix [tn fp; fn tp]:\")\n",
    "print([[tn_b, fp_b],[fn_b, tp_b]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56c3f38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static train size: (49988, 15) test size: (12497, 15)\n",
      "Static class distribution train: [21694 28294]\n",
      "Static class weights: {np.int64(0): np.float64(1.1521157923849912), np.int64(1): np.float64(0.8833674984095568)}\n",
      "\n",
      "=== Retrained Static RandomForest ===\n",
      "Best threshold  : 0.560\n",
      "Accuracy        : 0.9961\n",
      "Precision       : 0.9958\n",
      "Recall (TPR)    : 0.9973\n",
      "F1-score        : 0.9965\n",
      "ROC AUC         : 0.9996\n",
      "False Pos Rate  : 0.0055\n",
      "Confusion Matrix [tn fp; fn tp]:\n",
      "[[np.int64(5394), np.int64(30)], [np.int64(19), np.int64(7054)]]\n",
      "\n",
      "=== Static XGBoost Candidate ===\n",
      "Best threshold  : 0.780\n",
      "Accuracy        : 0.9966\n",
      "Precision       : 0.9980\n",
      "Recall (TPR)    : 0.9959\n",
      "F1-score        : 0.9970\n",
      "ROC AUC         : 0.9998\n",
      "False Pos Rate  : 0.0026\n",
      "Confusion Matrix [tn fp; fn tp]:\n",
      "[[np.int64(5410), np.int64(14)], [np.int64(29), np.int64(7044)]]\n"
     ]
    }
   ],
   "source": [
    "# ---------- Train/Test split for STATIC ----------\n",
    "Xs_train, Xs_test, ys_train, ys_test = train_test_split(\n",
    "    X_static, y_static,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_static\n",
    ")\n",
    "\n",
    "print(\"Static train size:\", Xs_train.shape, \"test size:\", Xs_test.shape)\n",
    "print(\"Static class distribution train:\", np.bincount(ys_train))\n",
    "\n",
    "# CLASS WEIGHTS for imbalance\n",
    "classes_s = np.unique(ys_train)\n",
    "class_weights_s = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=classes_s,\n",
    "    y=ys_train\n",
    ")\n",
    "cw_s = {cls: w for cls, w in zip(classes_s, class_weights_s)}\n",
    "print(\"Static class weights:\", cw_s)\n",
    "\n",
    "# --- 1. RandomForest baseline (retrained properly) ---\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=12,\n",
    "    class_weight=cw_s,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(Xs_train, ys_train)\n",
    "\n",
    "rf_proba = rf_model.predict_proba(Xs_test)[:,1]\n",
    "\n",
    "# tune threshold for RF\n",
    "thresholds = np.linspace(0.1, 0.9, 81)\n",
    "best_f1_rf = -1\n",
    "best_thr_rf = 0.5\n",
    "for t in thresholds:\n",
    "    preds_t = (rf_proba >= t).astype(int)\n",
    "    f1_t = f1_score(ys_test, preds_t, zero_division=0)\n",
    "    if f1_t > best_f1_rf:\n",
    "        best_f1_rf = f1_t\n",
    "        best_thr_rf = t\n",
    "\n",
    "rf_preds = (rf_proba >= best_thr_rf).astype(int)\n",
    "\n",
    "acc_rf  = accuracy_score(ys_test, rf_preds)\n",
    "prec_rf = precision_score(ys_test, rf_preds, zero_division=0)\n",
    "rec_rf  = recall_score(ys_test, rf_preds, zero_division=0)\n",
    "f1_rf   = f1_score(ys_test, rf_preds, zero_division=0)\n",
    "auc_rf  = roc_auc_score(ys_test, rf_proba)\n",
    "tn_rf, fp_rf, fn_rf, tp_rf = confusion_matrix(ys_test, rf_preds).ravel()\n",
    "fpr_rf = fp_rf / (fp_rf + tn_rf) if (fp_rf + tn_rf) > 0 else 0.0\n",
    "\n",
    "\n",
    "print(\"\\n=== Retrained Static RandomForest ===\")\n",
    "print(f\"Best threshold  : {best_thr_rf:.3f}\")\n",
    "print(f\"Accuracy        : {acc_rf:.4f}\")\n",
    "print(f\"Precision       : {prec_rf:.4f}\")\n",
    "print(f\"Recall (TPR)    : {rec_rf:.4f}\")\n",
    "print(f\"F1-score        : {f1_rf:.4f}\")\n",
    "print(f\"ROC AUC         : {auc_rf:.4f}\")\n",
    "print(f\"False Pos Rate  : {fpr_rf:.4f}\")\n",
    "print(\"Confusion Matrix [tn fp; fn tp]:\")\n",
    "print([[tn_rf, fp_rf],[fn_rf, tp_rf]])\n",
    "\n",
    "\n",
    "# --- 2. XGBoost candidate for static ---\n",
    "try:\n",
    "    xgb_model = XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=8,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        eval_metric='logloss',\n",
    "        scale_pos_weight= cw_s.get(1,1.0) / cw_s.get(0,1.0),  # weight malware class\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    xgb_model.fit(Xs_train, ys_train)\n",
    "\n",
    "    xgb_proba = xgb_model.predict_proba(Xs_test)[:,1]\n",
    "\n",
    "    # tune threshold for XGB\n",
    "    thresholds = np.linspace(0.1, 0.9, 81)\n",
    "    best_f1_xgb = -1\n",
    "    best_thr_xgb = 0.5\n",
    "    for t in thresholds:\n",
    "        preds_t = (xgb_proba >= t).astype(int)\n",
    "        f1_t = f1_score(ys_test, preds_t, zero_division=0)\n",
    "        if f1_t > best_f1_xgb:\n",
    "            best_f1_xgb = f1_t\n",
    "            best_thr_xgb = t\n",
    "\n",
    "    xgb_preds = (xgb_proba >= best_thr_xgb).astype(int)\n",
    "\n",
    "    acc_xgb  = accuracy_score(ys_test, xgb_preds)\n",
    "    prec_xgb = precision_score(ys_test, xgb_preds, zero_division=0)\n",
    "    rec_xgb  = recall_score(ys_test, xgb_preds, zero_division=0)\n",
    "    f1_xgb   = f1_score(ys_test, xgb_preds, zero_division=0)\n",
    "    auc_xgb  = roc_auc_score(ys_test, xgb_proba)\n",
    "    tn_xgb, fp_xgb, fn_xgb, tp_xgb = confusion_matrix(ys_test, xgb_preds).ravel()\n",
    "    fpr_xgb = fp_xgb / (fp_xgb + tn_xgb) if (fp_xgb + tn_xgb) > 0 else 0.0\n",
    "\n",
    "    print(\"\\n=== Static XGBoost Candidate ===\")\n",
    "    print(f\"Best threshold  : {best_thr_xgb:.3f}\")\n",
    "    print(f\"Accuracy        : {acc_xgb:.4f}\")\n",
    "    print(f\"Precision       : {prec_xgb:.4f}\")\n",
    "    print(f\"Recall (TPR)    : {rec_xgb:.4f}\")\n",
    "    print(f\"F1-score        : {f1_xgb:.4f}\")\n",
    "    print(f\"ROC AUC         : {auc_xgb:.4f}\")\n",
    "    print(f\"False Pos Rate  : {fpr_xgb:.4f}\")\n",
    "    print(\"Confusion Matrix [tn fp; fn tp]:\")\n",
    "    print([[tn_xgb, fp_xgb],[fn_xgb, tp_xgb]])\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Could not train XGBoost:\", e)\n",
    "    xgb_model = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "215d0067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved optimized models in C:\\Users\\richa\\OneDrive\\Documents\\FYP2\\models\\optimized\n"
     ]
    }
   ],
   "source": [
    "optimized_dir = project_root / \"models\" / \"optimized\"\n",
    "optimized_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save tuned behavioral CatBoost\n",
    "catboost_path = optimized_dir / \"behav_catboost_tuned.cbm\"\n",
    "cat_model.save_model(catboost_path)\n",
    "\n",
    "with open(optimized_dir / \"behav_threshold.json\", \"w\") as f:\n",
    "    import json\n",
    "    json.dump({\"best_threshold\": float(best_thr), \"note\": \"tuned F1 threshold\"}, f)\n",
    "\n",
    "# Save static models\n",
    "joblib.dump(rf_model, optimized_dir / \"static_rf_tuned.joblib\")\n",
    "joblib.dump({\"best_threshold\": float(best_thr_rf)}, optimized_dir / \"static_rf_threshold.json\")\n",
    "joblib.dump(list(X_static.columns), optimized_dir / \"static_rf_feature_names.joblib\")\n",
    "\n",
    "if 'xgb_model' in locals() and xgb_model is not None:\n",
    "    joblib.dump(xgb_model, optimized_dir / \"static_xgb_tuned.joblib\")\n",
    "    joblib.dump({\"best_threshold\": float(best_thr_xgb)}, optimized_dir / \"static_xgb_threshold.json\")\n",
    "    joblib.dump(list(X_static.columns), optimized_dir / \"static_xgb_feature_names.joblib\")\n",
    "\n",
    "print(\"Saved optimized models in\", optimized_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7de76724",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_processed/behav_baseline.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# pick a few rows from each processed dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m behav = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata_processed/behav_baseline.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.head(\u001b[32m5\u001b[39m)\n\u001b[32m      6\u001b[39m static = pd.read_parquet(\u001b[33m\"\u001b[39m\u001b[33mdata_processed/static_baseline.parquet\u001b[39m\u001b[33m\"\u001b[39m).head(\u001b[32m5\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# save them to CSVs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\richa\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\richa\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:258\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    257\u001b[39m     to_pandas_kwargs[\u001b[33m\"\u001b[39m\u001b[33msplit_blocks\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m path_or_handle, handles, filesystem = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    265\u001b[39m     pa_table = \u001b[38;5;28mself\u001b[39m.api.parquet.read_table(\n\u001b[32m    266\u001b[39m         path_or_handle,\n\u001b[32m    267\u001b[39m         columns=columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    270\u001b[39m         **kwargs,\n\u001b[32m    271\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\richa\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:141\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    131\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    134\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     path_or_handle = handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\richa\\OneDrive\\Documents\\FYP2\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data_processed/behav_baseline.parquet'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# pick a few rows from each processed dataset\n",
    "behav = pd.read_parquet(\"data_processed/behav_baseline.parquet\").head(5)\n",
    "static = pd.read_parquet(\"data_processed/static_baseline.parquet\").head(5)\n",
    "\n",
    "# save them to CSVs\n",
    "behav.to_csv(\"demo_behav_sample.csv\", index=False)\n",
    "static.to_csv(\"demo_static_sample.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ad41e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behav_path: C:\\Users\\richa\\OneDrive\\Documents\\FYP2\\data_processed\\behav_baseline.parquet\n",
      "static_path: C:\\Users\\richa\\OneDrive\\Documents\\FYP2\\data_processed\\static_baseline.parquet\n",
      "Saved demo_behav_sample.csv and demo_static_sample.csv in project root ✅\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path().resolve().parent  # this assumes you're in FYP2/notebooks right now\n",
    "\n",
    "behav_path = project_root / \"data_processed\" / \"behav_baseline.parquet\"\n",
    "static_path = project_root / \"data_processed\" / \"static_baseline.parquet\"\n",
    "\n",
    "print(\"behav_path:\", behav_path)\n",
    "print(\"static_path:\", static_path)\n",
    "\n",
    "behav = pd.read_parquet(behav_path).head(5)\n",
    "static = pd.read_parquet(static_path).head(5)\n",
    "\n",
    "behav.to_csv(project_root / \"demo_behav_sample.csv\", index=False)\n",
    "static.to_csv(project_root / \"demo_static_sample.csv\", index=False)\n",
    "\n",
    "print(\"Saved demo_behav_sample.csv and demo_static_sample.csv in project root ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d35cdfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys, json, numpy as np, pandas as pd\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.append(str(project_root / \"src\"))\n",
    "\n",
    "from utils import build_behav_features  # uses your utils.py\n",
    "\n",
    "# load raw parquet\n",
    "df_behav_raw = pd.read_parquet(project_root / \"data_processed\" / \"behav_baseline.parquet\")\n",
    "\n",
    "# engineer features exactly like inference does\n",
    "df_b = build_behav_features(df_behav_raw)\n",
    "\n",
    "# label (same as before)\n",
    "y_cb = df_b[\"is_wannacry\"].astype(int) if \"is_wannacry\" in df_b.columns else \\\n",
    "       df_behav_raw[\"Prediction\"].apply(lambda x: 1 if x == \"A\" else 0).astype(int)\n",
    "\n",
    "# define columns used by model (order matters!)\n",
    "cat_cols = [\"Protocol\",\"Flag\",\"Threats\",\"ProtoFlag\",\"PortBucket\",\"has_BTC\",\"has_USD\"]\n",
    "num_cols = [\"Time\",\"Clusters\",\"Netflow_Bytes\",\"BTC\",\"USD\"]  # <— order fixed\n",
    "use_cols = num_cols + cat_cols\n",
    "\n",
    "# assemble X in that exact order\n",
    "X_cb = df_b.reindex(columns=use_cols, fill_value=0).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0be49e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen threshold: 0.6741265375003658\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "Xtr_cb, Xte_cb, ytr_cb, yte_cb = train_test_split(\n",
    "    X_cb, y_cb, test_size=0.20, stratify=y_cb, random_state=42\n",
    ")\n",
    "\n",
    "# cat feature indices (positions in use_cols)\n",
    "cat_idx = [X_cb.columns.get_loc(c) for c in cat_cols]\n",
    "\n",
    "# class weighting\n",
    "neg = int((ytr_cb == 0).sum()); pos = int((ytr_cb == 1).sum())\n",
    "spw = neg / max(1, pos)\n",
    "\n",
    "train_pool = Pool(Xtr_cb, ytr_cb, cat_features=cat_idx)\n",
    "val_pool   = Pool(Xte_cb, yte_cb, cat_features=cat_idx)\n",
    "\n",
    "cb = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    depth=6,\n",
    "    learning_rate=0.05,\n",
    "    loss_function=\"Logloss\",\n",
    "    eval_metric=\"PRAUC\",\n",
    "    scale_pos_weight=spw * 1.2,\n",
    "    random_seed=42,\n",
    "    verbose=False,\n",
    "    od_type=\"Iter\",\n",
    "    od_wait=50\n",
    ")\n",
    "cb.fit(train_pool, eval_set=val_pool, use_best_model=True)\n",
    "\n",
    "proba_cb = cb.predict_proba(val_pool)[:, 1]\n",
    "prec, rec, thr = precision_recall_curve(yte_cb, proba_cb)\n",
    "f1 = 2*(prec*rec)/(prec+rec+1e-12)\n",
    "best_idx = int(np.nanargmax(f1))\n",
    "best_thr = float(thr[best_idx]) if best_idx < len(thr) else 0.5\n",
    "\n",
    "print(\"Chosen threshold:\", best_thr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "895a84e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Behavioral model + feature names saved correctly.\n"
     ]
    }
   ],
   "source": [
    "optimized_dir = project_root / \"models\" / \"optimized\"\n",
    "optimized_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save CatBoost model\n",
    "cb.save_model(optimized_dir / \"behav_catboost_tuned.cbm\")\n",
    "\n",
    "# Save threshold\n",
    "with open(optimized_dir / \"behav_threshold.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"best_threshold\": float(best_thr)}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Save the exact feature order and types\n",
    "feat_meta = {\n",
    "    \"use_cols\": list(X_cb.columns),\n",
    "    \"cat_cols\": cat_cols,\n",
    "    \"num_cols\": num_cols\n",
    "}\n",
    "with open(optimized_dir / \"behav_feature_names.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(feat_meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ Behavioral model + feature names saved correctly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c07e56fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimized_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[43moptimized_dir\u001b[49m / \u001b[33m\"\u001b[39m\u001b[33mstatic_xgb_threshold.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      2\u001b[39m     json.dump({\u001b[33m\"\u001b[39m\u001b[33mbest_threshold\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(best_thr_xgb)}, f, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m, indent=\u001b[32m2\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'optimized_dir' is not defined"
     ]
    }
   ],
   "source": [
    "with open(optimized_dir / \"static_xgb_threshold.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"best_threshold\": float(best_thr_xgb)}, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca74ce4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized_dir = C:\\Users\\richa\\OneDrive\\Documents\\FYP2\\models\\optimized\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "project_root = Path().resolve().parent   # <-- same as before\n",
    "optimized_dir = project_root / \"models\" / \"optimized\"\n",
    "optimized_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"optimized_dir =\", optimized_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f03a914",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# tune threshold for XGB\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m thresholds = \u001b[43mnp\u001b[49m.linspace(\u001b[32m0.1\u001b[39m, \u001b[32m0.9\u001b[39m, \u001b[32m81\u001b[39m)\n\u001b[32m      3\u001b[39m best_f1_xgb = -\u001b[32m1\u001b[39m\n\u001b[32m      4\u001b[39m best_thr_xgb = \u001b[32m0.5\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# tune threshold for XGB\n",
    "thresholds = np.linspace(0.1, 0.9, 81)\n",
    "best_f1_xgb = -1\n",
    "best_thr_xgb = 0.5\n",
    "for t in thresholds:\n",
    "    preds_t = (xgb_proba >= t).astype(int)\n",
    "    f1_t = f1_score(ys_test, preds_t, zero_division=0)\n",
    "    if f1_t > best_f1_xgb:\n",
    "        best_f1_xgb = f1_t\n",
    "        best_thr_xgb = t\n",
    "\n",
    "print(\"best_thr_xgb =\", best_thr_xgb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
